{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate prediction intervals associated with a time series forecast. For an example using MapieTimeSeriesRegressor, see  \n",
    "mapie documentation: https://mapie.readthedocs.io/en/latest/examples_regression/4-tutorials/plot_ts-tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "skip_warnings = True\n",
    "if skip_warnings:\n",
    "    warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "    # Suppress specific warning related to the deprecated binary model format\n",
    "    warnings.filterwarnings(\"ignore\", message=\".*deprecated binary model format.*\")\n",
    "    # Suppress specific warning from EconML\n",
    "    warnings.filterwarnings(\"ignore\", message=\"Co-variance matrix is underdetermined. Inference will be invalid!\")\n",
    "    # Suppress specific warning from MAPIE\n",
    "    warnings.filterwarnings(\"ignore\", message=\"WARNING: The predictions are ill-sorted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import random, copy\n",
    "from functools import wraps\n",
    "import datetime as dt\n",
    "import json\n",
    "import itertools\n",
    "from packaging import version\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import colormaps\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from scipy.stats import yeojohnson, boxcox\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, OrdinalEncoder, LabelBinarizer\n",
    "from sklearn.preprocessing import QuantileTransformer, Normalizer, RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from category_encoders.woe import WOEEncoder\n",
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "from category_encoders.glmm import GLMMEncoder\n",
    "from category_encoders.wrapper import PolynomialWrapper\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import FunctionTransformer, PowerTransformer, KBinsDiscretizer\n",
    "from imblearn import FunctionSampler\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklego.preprocessing import ColumnSelector\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from imblearn.pipeline import Pipeline as PipelineImb\n",
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.utils import shuffle, check_array\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor, XGBRFRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn_quantile import RandomForestQuantileRegressor, ExtraTreesQuantileRegressor, KNeighborsQuantileRegressor\n",
    "from mapie.regression import MapieRegressor\n",
    "from mapie.regression import MapieQuantileRegressor\n",
    "from mapie.subsample import Subsample\n",
    "from mapie.metrics import coverage_width_based, regression_coverage_score, regression_mean_width_score, regression_mwi_score\n",
    "from mapie.regression import MapieTimeSeriesRegressor\n",
    "from mapie.subsample import BlockBootstrap\n",
    "import MWIS_metric\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "\n",
    "from econml.dml import LinearDML, CausalForestDML\n",
    "from econml.orf import DMLOrthoForest\n",
    "from sklearn.preprocessing import KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import copy_dataframe, format_time_column, merge_onehot_columns, add_interaction_columns, replace_inf_with_nan, drop_rows_with_nan\n",
    "from common import change_column_names, strip_blanks_column_names, move_columns_to_end_of_dataframe, add_date_component_columns\n",
    "from common import create_bins, get_shap_values_from_array, split_dataset_in_train_calibrate_test, get_regression_metrics\n",
    "from common import set_sample_weights, set_baseline_model\n",
    "\n",
    "from common import YeoJohnsonTargetTransformer, LogTargetTransformer, Log1pTransformerWithShift, RuleBasedModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import path names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import DATA_DIR, EXTERNAL_DIR, INTERIM_DIR, OUTPUT_DIR, FIGURES_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "sklearn_display = 'diagram'  # 'diagram' or 'text'\n",
    "set_config(display=sklearn_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "# Get initial Matplotlib figure settings\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "initial_settings = plt.rcParams.copy()  # to avoid shap overruling initial plot settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_settings import seed_value\n",
    "\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_name = EXTERNAL_DIR\n",
    "\n",
    "file_name = 'data_training_energy_euro_V2.csv'\n",
    "full_path = path_name / file_name\n",
    "print(f\"LOAD: {str(full_path)}\\n\")\n",
    "data_raw = pd.read_csv(full_path)\n",
    "\n",
    "data_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_settings import time_col, categoricals_dict, colnames_dict\n",
    "\n",
    "print(f\"Name of time column:\")\n",
    "print(f\"    '{time_col}'\")\n",
    "print(f\"Onehot encoded columns:\")\n",
    "for col in categoricals_dict.keys():\n",
    "    print(f\"  - '{col}': {categoricals_dict[col]}\")\n",
    "print(f\"New names for columns:\")\n",
    "for col in colnames_dict.keys():\n",
    "    print(f\"  - '{col}': '{colnames_dict[col]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import copy_dataframe, format_time_column, merge_onehot_columns, add_interaction_columns, replace_inf_with_nan, drop_rows_with_nan, reset_index_dataframe\n",
    "from common import change_column_names, strip_blanks_column_names, move_columns_to_end_of_dataframe, add_date_component_columns, make_timeseries_equal_length\n",
    "from common import add_sin_cos_feature_columns, add_lagged_columns, add_moving_average_columns, add_diff_target, drop_days_from_series\n",
    "from model_settings import add_lagged_target_to_model, target_max_lags, target, add_lagged_swan_predictions_to_model, swan_max_lags\n",
    "from model_settings import add_moving_average_target_to_model, add_moving_average_swan_to_model, ma_window\n",
    "from model_settings import periods_for_bias_correction, columns_not_in_model\n",
    "from data_settings import dates_to_drop\n",
    "\n",
    "\n",
    "data_clean = (data_raw\n",
    "    .pipe(copy_dataframe)\n",
    "    .pipe(format_time_column, dates_col=time_col, format='%Y-%m-%d %H:%M:%S')\n",
    "    .pipe(add_date_component_columns, dates_col=time_col)\n",
    "    .pipe(merge_onehot_columns, columns_dict=categoricals_dict)\n",
    "    .pipe(add_interaction_columns)\n",
    "    .pipe(replace_inf_with_nan)\n",
    "    .pipe(add_sin_cos_feature_columns, dates_col=time_col, period=periods_for_bias_correction)\n",
    "    .pipe(change_column_names, columns_rename=colnames_dict)\n",
    "    .pipe(strip_blanks_column_names)\n",
    "    .pipe(add_diff_target, column_name=target, groupby_columns=['Location','Frequency'], sortby_column='Time')\n",
    "    .pipe(add_lagged_columns, column_name=target, groupby_columns=['Location','Frequency'], sortby_column='Time', add_column=add_lagged_target_to_model, lags=target_max_lags, columns_to_drop=columns_not_in_model)\n",
    "    .pipe(add_lagged_columns, column_name='SWAN prediction', groupby_columns=['Location','Frequency'], sortby_column='Time', add_column=add_lagged_swan_predictions_to_model, lags=swan_max_lags, columns_to_drop=columns_not_in_model)\n",
    "    .pipe(add_moving_average_columns, column_name=target, groupby_columns=['Location','Frequency'], sortby_column='Time', add_column=add_moving_average_target_to_model, window=ma_window, shift_series=True, columns_to_drop=columns_not_in_model)\n",
    "    .pipe(add_moving_average_columns, column_name='SWAN prediction', groupby_columns=['Location','Frequency'], sortby_column='Time', add_column=add_moving_average_swan_to_model, window=ma_window, shift_series=False, columns_to_drop=columns_not_in_model)\n",
    "    .pipe(drop_days_from_series, dates_to_drop=dates_to_drop)\n",
    "    .pipe(drop_rows_with_nan)\n",
    "    # .pipe(move_columns_to_end_of_dataframe, column_names=['value','output'])\n",
    "    # .pipe(make_timeseries_equal_length, dates_col='Time', locations_col='Location', frequencies_col='Frequency')\n",
    "    .pipe(reset_index_dataframe, drop_original_index=True)\n",
    ")\n",
    "\n",
    "print('')\n",
    "data_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_settings import train_size, calib_size, test_size, target, locations_to_use, frequencies_to_use, columns_not_in_model\n",
    "from model_settings import split_method, binned_target_log, binned_target_nbins, leave_one_out_location, shuffle_rows\n",
    "from model_settings import imputation_method, scaler_type, scaler_quantile_nquantiles, scaler_rowwise_norm\n",
    "from model_settings import use_categorical_encoding, use_ordinal_encoding, use_numeric_transform, pca_transform_numeric, use_target_transform\n",
    "from model_settings import ordinal_columns, continuous_to_ordinal_transform_nbins, categorical_columns, categorical_encode_method, numeric_transform_method\n",
    "from model_settings import target_transform_method, target_yeojohnson_lambda, target_yeojohnson_nan_percentile\n",
    "from model_settings import use_sample_weights, fit_weight_tails, fit_quantiles_tails\n",
    "from model_settings import set_monotonic_constraints, features_monotone_increasing, features_monotone_decreasing, monotone_constraints_method_lgbm\n",
    "\n",
    "print(f\"train_size: {train_size}\")\n",
    "print(f\"calib_size: {calib_size}\")\n",
    "print(f\"test_size: {test_size}\")\n",
    "print(f\"target: {target}\")\n",
    "print(f\"locations_to_use: {locations_to_use}\")\n",
    "print(f\"frequencies_to_use: {frequencies_to_use}\")\n",
    "print(f\"split_method: {split_method}\")\n",
    "print(f\"binned_target_log: {binned_target_log}\")\n",
    "print(f\"binned_target_nbins: {binned_target_nbins}\")\n",
    "print(f\"leave_one_out_location: {leave_one_out_location}\")\n",
    "print(f\"shuffle_rows: {shuffle_rows}\")\n",
    "print(f\"imputation_method: {imputation_method}\")\n",
    "print(f\"scaler_type: {scaler_type}\")\n",
    "print(f\"scaler_quantile_nquantiles: {scaler_quantile_nquantiles}\")\n",
    "print(f\"scaler_rowwise_norm: {scaler_rowwise_norm}\")\n",
    "print(f\"columns_not_in_model: {columns_not_in_model}\")\n",
    "print(f\"use_categorical_encoding: {use_categorical_encoding}\")\n",
    "print(f\"use_ordinal_encoding: {use_ordinal_encoding}\")\n",
    "print(f\"use_numeric_transform: {use_numeric_transform}\")\n",
    "print(f\"pca_transform_numeric: {pca_transform_numeric}\")\n",
    "print(f\"use_target_transform: {use_target_transform}\")\n",
    "print(f\"ordinal_columns: {ordinal_columns}\")\n",
    "print(f\"continuous_to_ordinal_transform_nbins: {continuous_to_ordinal_transform_nbins}\")\n",
    "print(f\"categorical_columns: {categorical_columns}\")\n",
    "print(f\"categorical_encode_method: {categorical_encode_method}\")\n",
    "print(f\"numeric_transform_method: {numeric_transform_method}\")\n",
    "print(f\"target_transform_method: {target_transform_method}\")\n",
    "print(f\"target_yeojohnson_lambda: {target_yeojohnson_lambda}\")\n",
    "print(f\"target_yeojohnson_nan_percentile: {target_yeojohnson_nan_percentile}\")\n",
    "print(f\"use_sample_weights: {use_sample_weights}\")\n",
    "print(f\"fit_weight_tails: {fit_weight_tails}\")\n",
    "print(f\"fit_quantiles_tails: {fit_quantiles_tails}\")\n",
    "print(f\"set_monotonic_constraints: {set_monotonic_constraints}\")\n",
    "print(f\"features_monotone_increasing: {features_monotone_increasing}\")\n",
    "print(f\"features_monotone_decreasing: {features_monotone_decreasing}\")\n",
    "# print(f\"monotone_constraints_method_lgbm: {monotone_constraints_method_lgbm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train(-calibrate)-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_settings import train_size, calib_size, test_size, target, locations_to_use, frequencies_to_use, columns_not_in_model\n",
    "from model_settings import split_method, split_dates_table, binned_target_log, binned_target_nbins, leave_one_out_location, shuffle_rows\n",
    "from model_settings import imputation_method, scaler_type, scaler_quantile_nquantiles, scaler_rowwise_norm\n",
    "from model_settings import use_categorical_encoding, use_ordinal_encoding, use_numeric_transform, pca_transform_numeric, use_target_transform\n",
    "from model_settings import ordinal_columns, continuous_to_ordinal_transform_nbins, categorical_columns, categorical_encode_method, numeric_transform_method\n",
    "from model_settings import target_transform_method, target_yeojohnson_lambda, target_yeojohnson_nan_percentile\n",
    "from model_settings import use_sample_weights, fit_weight_tails, fit_quantiles_tails\n",
    "from model_settings import set_monotonic_constraints, features_monotone_increasing, features_monotone_decreasing, monotone_constraints_method_lgbm\n",
    "\n",
    "# override split method\n",
    "# split_method = 'timeseries_split'\n",
    "# split_method = 'leave_one_out'\n",
    "split_method = 'dates_table'\n",
    "\n",
    "# override split ratios\n",
    "train_size = 0.6\n",
    "calib_size = 0.2\n",
    "test_size = 0.2\n",
    "\n",
    "X = data_clean.copy().loc[(data_clean['Location'].isin(locations_to_use)) & (data_clean['Frequency'].isin(frequencies_to_use)), data_clean.drop(columns=target).columns]\n",
    "y = data_clean.copy().loc[(data_clean['Location'].isin(locations_to_use)) & (data_clean['Frequency'].isin(frequencies_to_use)), target]\n",
    "\n",
    "train_data, calib_data, test_data = split_dataset_in_train_calibrate_test(\n",
    "    X, y, target, split_ratios=(train_size, calib_size, test_size), \n",
    "    method=split_method, seed_value=seed_value, loo_loc=leave_one_out_location,\n",
    "    dates_table=split_dates_table, shuffle_rows=shuffle_rows,\n",
    ")\n",
    "\n",
    "X_train = train_data['X']\n",
    "y_train = train_data['y']\n",
    "X_calib = calib_data['X']\n",
    "y_calib = calib_data['y']\n",
    "X_test = test_data['X']\n",
    "y_test = test_data['y']\n",
    "# train_index = train_data['index']\n",
    "# calib_index = calib_data['index']\n",
    "# test_index = test_data['index']\n",
    "\n",
    "# digits = 3\n",
    "# print(f\"Train size:       {np.round(X_train.shape[0] / (X_train.shape[0] + X_calib.shape[0] + X_test.shape[0]), digits)}\")\n",
    "# if calib_size > 0:\n",
    "#     print(f\"Calibration size: {np.round(X_calib.shape[0] / (X_train.shape[0] + X_calib.shape[0] + X_test.shape[0]), digits)}\")\n",
    "# print(f\"Test size:        {np.round(X_test.shape[0] / (X_train.shape[0] + X_calib.shape[0] + X_test.shape[0]), digits)}\")\n",
    "\n",
    "digits = 3\n",
    "print(f\"Train size:       {X_train.shape[0]}\")\n",
    "if calib_size > 0:\n",
    "    print(f\"Calibration size: {X_calib.shape[0]}\")\n",
    "print(f\"Test size:        {X_test.shape[0]}\")\n",
    "\n",
    "X_new, y_new = X_test.copy(), y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of time series per location (train data)')\n",
    "for loc_name in X['Location'].unique():\n",
    "    print(f\"  {loc_name}: {X_train.loc[X.Location==loc_name,'Time'].min()} - {X_train.loc[X.Location==loc_name,'Time'].max()}\")\n",
    "if calib_size > 0:\n",
    "    print('\\nLength of time series per location (calibration data)')\n",
    "    for loc_name in X['Location'].unique():\n",
    "        print(f\"  {loc_name}: {X_calib.loc[X.Location==loc_name,'Time'].min()} - {X_calib.loc[X.Location==loc_name,'Time'].max()}\")\n",
    "print('\\nLength of time series per location (test data)')\n",
    "for loc_name in X['Location'].unique():\n",
    "    print(f\"  {loc_name}: {X_test.loc[X.Location==loc_name,'Time'].min()} - {X_test.loc[X.Location==loc_name,'Time'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Range of values per location (train data)')\n",
    "for loc_name in X['Location'].unique():\n",
    "    print(f\"  {loc_name}: {np.round(y_train.loc[X_train.loc[X.Location==loc_name].index].min(),3)} - {np.round(y_train.loc[X_train.loc[X.Location==loc_name].index].max(),3)}\")\n",
    "if calib_size > 0:\n",
    "    print('\\nRange of values per location (calibration data)')\n",
    "    for loc_name in X['Location'].unique():\n",
    "        print(f\"  {loc_name}: {np.round(y_calib.loc[X_calib.loc[X.Location==loc_name].index].min(),3)} - {np.round(y_calib.loc[X_calib.loc[X.Location==loc_name].index].max(),3)}\")\n",
    "print('\\nRange of values per location (test data)')\n",
    "for loc_name in X['Location'].unique():\n",
    "    print(f\"  {loc_name}: {np.round(y_test.loc[X_test.loc[X.Location==loc_name].index].min(),3)} - {np.round(y_test.loc[X_test.loc[X.Location==loc_name].index].max(),3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_model_pipeline():\n",
    "    global rgr_mdl\n",
    "\n",
    "    # Define lists to hold preprocessing steps\n",
    "    num_preprocessor, cat_preprocessor, discrete2ord_preprocessor, continuous2ord_preprocessor, drop_col_preprocessor = [], [], [], [], []\n",
    "\n",
    "    # Feature type lists\n",
    "    drop_var_names = columns_not_in_model\n",
    "    discrete2ord_var_names = [col for col in Xtrain_columns if col in discrete_to_ordinal and col not in drop_var_names] if use_ordinal_encoding else []\n",
    "    continuous2ord_var_names = [col for col in Xtrain_columns if col in continuous_to_ordinal and col not in drop_var_names] if use_ordinal_encoding else []\n",
    "    cat_var_names = [col for col in Xtrain_columns if col in categorical_columns and col not in drop_var_names] if use_categorical_encoding else []\n",
    "    num_var_names = [col for col in Xtrain_columns if col not in cat_var_names and col not in discrete2ord_var_names and col not in continuous2ord_var_names and col not in drop_var_names]\n",
    "\n",
    "    # Get column indexes of feature types\n",
    "    num_vars_idx = [Xtrain_columns.index(col) for col in num_var_names if col in Xtrain_columns]\n",
    "    cat_vars_idx = [Xtrain_columns.index(col) for col in cat_var_names if col in Xtrain_columns]\n",
    "    discrete2ord_vars_idx = [Xtrain_columns.index(col) for col in discrete2ord_var_names if col in Xtrain_columns]\n",
    "    continuous2ord_vars_idx = [Xtrain_columns.index(col) for col in continuous2ord_var_names if col in Xtrain_columns]\n",
    "    drop_vars_idx = [Xtrain_columns.index(col) for col in drop_var_names if col in Xtrain_columns]\n",
    "\n",
    "    # Drop features\n",
    "    drop_col_preprocessor.append(('column_dropper', FunctionTransformer(drop_columns_by_name, kw_args={'columns_to_drop': drop_var_names}, validate=False)))\n",
    "\n",
    "    # Impute missing values\n",
    "    if imputation_method in ['median', 'mean']:\n",
    "        num_preprocessor.append(('imputer', SimpleImputer(strategy=imputation_method)))\n",
    "        if use_ordinal_encoding:\n",
    "            if discrete2ord_vars_idx:\n",
    "                discrete2ord_preprocessor.append(('imputer', SimpleImputer(strategy=imputation_method)))\n",
    "            if continuous2ord_vars_idx:\n",
    "                continuous2ord_preprocessor.append(('imputer', SimpleImputer(strategy=imputation_method)))\n",
    "    elif imputation_method == 'knn':\n",
    "        num_preprocessor.append(('imputer', KNNImputer()))\n",
    "        if use_ordinal_encoding:\n",
    "            if discrete2ord_vars_idx:\n",
    "                discrete2ord_preprocessor.append(('imputer', KNNImputer()))\n",
    "            if continuous2ord_vars_idx:\n",
    "                continuous2ord_preprocessor.append(('imputer', KNNImputer()))\n",
    "    if imputation_method != 'drop':\n",
    "        if use_categorical_encoding and cat_vars_idx:\n",
    "            if categorical_encode_method in ['WOE', 'CatBoost']:\n",
    "                cat_preprocessor.append(('astype_str', FunctionTransformer(safe_convert_to_str, validate=False)))  # WOE will transform all string columns\n",
    "            cat_preprocessor.append(('imputer', SimpleImputer(strategy='most_frequent')))\n",
    "\n",
    "    # Transform numeric feature values\n",
    "    if use_numeric_transform:\n",
    "        if numeric_transform_method == 'log':\n",
    "            num_preprocessor.append(('transformer', Log1pTransformerWithShift()))\n",
    "        elif numeric_transform_method == 'signed_log':\n",
    "            num_preprocessor.append(('transformer', FunctionTransformer(func=signed_log1p_transform, inverse_func=inv_signed_log1p_transform, validate=False)))\n",
    "        elif numeric_transform_method == 'yeojohnson':\n",
    "            num_preprocessor.append(('transformer', PowerTransformer(method='yeo-johnson')))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported numeric_transform_method for numeric features: {numeric_transform_method}\")\n",
    "\n",
    "    # Onehot encode categorical features\n",
    "    if use_categorical_encoding and cat_vars_idx:\n",
    "        if categorical_encode_method == 'OHE':\n",
    "            if version.parse(sklearn.__version__) >= version.parse('1.2'):\n",
    "                cat_preprocessor.append(('onehot_encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False)))\n",
    "            else:\n",
    "                cat_preprocessor.append(('onehot_encoder', OneHotEncoder(handle_unknown='ignore', sparse=False)))\n",
    "        elif categorical_encode_method == 'target':\n",
    "            cat_preprocessor.append(('target_encoder', TargetEncoder(verbose=0, handle_unknown='value', handle_missing='value', return_df=False, drop_invariant=False)))\n",
    "        elif categorical_encode_method == 'GLMM':\n",
    "            cat_preprocessor.append(('glmm_encoder', GLMMEncoder(verbose=0, handle_unknown='value', handle_missing='value', return_df=False, drop_invariant=False)))\n",
    "        elif categorical_encode_method == 'WOE':\n",
    "            cat_preprocessor.append(('woe_encoder', WOEEncoder(verbose=0, handle_unknown='value', handle_missing='value', return_df=False, drop_invariant=False)))\n",
    "        elif categorical_encode_method == 'CatBoost':\n",
    "            cat_preprocessor.append(('catboost_encoder', CatBoostEncoder(verbose=0, handle_unknown='value', handle_missing='value', return_df=False, drop_invariant=False)))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported categorical_encode_method for features: {categorical_encode_method}\")\n",
    "\n",
    "    # Ordinal encode selected numeric features\n",
    "    if use_ordinal_encoding:\n",
    "        if discrete2ord_vars_idx:\n",
    "            discrete2ord_preprocessor.append(('ordinal_encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)))\n",
    "        if continuous2ord_vars_idx:\n",
    "            nbins_discretizer = [continuous_to_ordinal_transform_nbins.get(name, 10) for name in continuous2ord_var_names]\n",
    "            continuous2ord_preprocessor.append(('ordinal_discretizer', KBinsDiscretizer(n_bins=nbins_discretizer, encode='ordinal', strategy='uniform')))\n",
    "\n",
    "    # Scale numeric (and ordinal) feature values\n",
    "    scl_type = models[mdl_type].get('scaler', None)\n",
    "    add_pca_to_pipe = models[mdl_type].get('pca_transform', None)\n",
    "    scaler = None\n",
    "    if scl_type == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scl_type == 'minmax' and not pca_transform_numeric:\n",
    "        scaler = MinMaxScaler()\n",
    "    elif scl_type == 'quantile' and not pca_transform_numeric:\n",
    "        scaler = QuantileTransformer(n_quantiles=scaler_quantile_nquantiles)\n",
    "    elif scl_type == 'robust' and not pca_transform_numeric:\n",
    "        scaler = RobustScaler()\n",
    "    elif scl_type == 'rowwise' and not pca_transform_numeric:\n",
    "        scaler = Normalizer(norm=scaler_rowwise_norm)\n",
    "    if scaler:\n",
    "        num_preprocessor.append(('scaler', scaler))\n",
    "        if not use_categorical_encoding and cat_vars_idx:\n",
    "            cat_preprocessor.append(('scaler', scaler))\n",
    "        if use_ordinal_encoding:\n",
    "            if discrete2ord_vars_idx:\n",
    "                discrete2ord_preprocessor.append(('scaler', scaler))\n",
    "            if continuous2ord_vars_idx:\n",
    "                continuous2ord_preprocessor.append(('scaler', scaler))\n",
    "    if pca_transform_numeric:\n",
    "        num_preprocessor.append(('scaler', StandardScaler()))\n",
    "        num_preprocessor.append(('pca', PCA(n_components=len(num_var_names))))\n",
    "\n",
    "    # Preprocessor part of pipeline\n",
    "    preprocessor_steps = []\n",
    "    if num_preprocessor:\n",
    "        num_transformer = Pipeline(num_preprocessor)\n",
    "        preprocessor_steps.append(('numeric', num_transformer, num_vars_idx))\n",
    "    if discrete2ord_preprocessor:\n",
    "        discrete2ord_transformer = Pipeline(discrete2ord_preprocessor)\n",
    "        preprocessor_steps.append(('discrete_to_ordinal', discrete2ord_transformer, discrete2ord_vars_idx))\n",
    "    if continuous2ord_preprocessor:\n",
    "        continuous2ord_transformer = Pipeline(continuous2ord_preprocessor)\n",
    "        preprocessor_steps.append(('continuous_to_ordinal', continuous2ord_transformer, continuous2ord_vars_idx))\n",
    "    if cat_preprocessor:\n",
    "        cat_transformer = Pipeline(cat_preprocessor)\n",
    "        preprocessor_steps.append(('categorical', cat_transformer, cat_vars_idx))\n",
    "    if drop_col_preprocessor:\n",
    "        drop_col_transformer = Pipeline(drop_col_preprocessor)\n",
    "        preprocessor_steps.append(('drop_columns', drop_col_transformer, drop_vars_idx))\n",
    "    preprocessor = ColumnTransformer(transformers=preprocessor_steps, remainder='passthrough')\n",
    "\n",
    "    # Set random state of classifier\n",
    "    if 'random_state' in rgr_mdl.get_params():\n",
    "        rgr_mdl.set_params(random_state=seed_value)\n",
    "    elif 'random_seed' in rgr_mdl.get_params():\n",
    "        rgr_mdl.set_params(random_seed=seed_value)\n",
    "\n",
    "    # Set monotone constraints (only works for gradient-boosting models and linear regression models)\n",
    "    if set_monotonic_constraints and not pca_transform_numeric:\n",
    "        monotone_constraints, monotone_columns = monotonic_constraints(\n",
    "            num_vars=num_var_names,\n",
    "            ord_vars=discrete2ord_var_names + continuous2ord_var_names, \n",
    "            cat_vars=cat_var_names, \n",
    "            cat_encode=categorical_encode_method, \n",
    "            X=X_train, \n",
    "            features_increasing=features_monotone_increasing, \n",
    "            features_decreasing=features_monotone_decreasing,\n",
    "        )\n",
    "        if mdl_type == 'LGBM':\n",
    "            rgr_mdl.set_params(monotone_constraints=monotone_constraints, monotone_constraints_method=monotone_constraints_method_lgbm)\n",
    "        elif mdl_type == 'CatB':\n",
    "            rgr_mdl.set_params(monotone_constraints=monotone_constraints)\n",
    "        elif mdl_type == 'HGB':\n",
    "            rgr_mdl.set_params(monotonic_cst=monotone_constraints)\n",
    "        elif mdl_type in ['XGB', 'XGB-RF']:\n",
    "            rgr_mdl.set_params(monotone_constraints=tuple(monotone_constraints))\n",
    "        elif mdl_type in ['LR', 'Ridge', 'Lasso']:\n",
    "            rgr_mdl = ConstrainedLinearRegression()\n",
    "            min_coef = np.repeat(-np.inf, len(monotone_constraints))\n",
    "            max_coef = np.repeat(np.inf, len(monotone_constraints))\n",
    "            for j, col in enumerate(monotone_columns):\n",
    "                if monotone_constraints[j] < 0:\n",
    "                    max_coef[j] = 0.\n",
    "                elif monotone_constraints[j] > 0:\n",
    "                    min_coef[j] = 0.\n",
    "            if mdl_type == 'Ridge':\n",
    "                rgr_mdl.set_params(max_coef=max_coef, min_coef=min_coef, ridge=True)\n",
    "            elif mdl_type == 'Lasso':\n",
    "                rgr_mdl.set_params(max_coef=max_coef, min_coef=min_coef, lasso=True)\n",
    "            else:\n",
    "                rgr_mdl.set_params(max_coef=max_coef, min_coef=min_coef)\n",
    "\n",
    "    # Transform target variable\n",
    "    if use_target_transform:\n",
    "        if target_transform_method == 'yeojohnson':\n",
    "            rgr_mdl = TransformedTargetRegressor(regressor=rgr_mdl, transformer=YeoJohnsonTargetTransformer(lmbda=target_yeojohnson_lambda, replace_nan_percentile=target_yeojohnson_nan_percentile))\n",
    "        elif target_transform_method == 'log':\n",
    "            rgr_mdl = TransformedTargetRegressor(regressor=rgr_mdl, transformer=LogTargetTransformer())\n",
    "\n",
    "    # Add all steps to final pipeline\n",
    "    steps = [('preprocessor', preprocessor)] if preprocessor_steps else []\n",
    "    steps.append(('model', rgr_mdl))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "# custom function for log-transform\n",
    "def log_transform(x):\n",
    "    return np.log1p(x)\n",
    "\n",
    "def inv_log_transform(x):\n",
    "    return np.expm1(x)\n",
    "\n",
    "\n",
    "# Custom function for signed log1p-transform\n",
    "def signed_log1p_transform(x):\n",
    "    return np.sign(x) * np.log1p(np.abs(x))\n",
    "\n",
    "def inv_signed_log1p_transform(x):\n",
    "    return np.sign(x) * np.expm1(np.abs(x))\n",
    "\n",
    "\n",
    "# custom function to convert numeric values to strings\n",
    "def safe_convert_to_str(X):\n",
    "    return X.applymap(lambda x: str(x) if x is not np.nan else x)\n",
    "\n",
    "\n",
    "# custom function to drop columns by name\n",
    "def drop_columns_by_name(X, columns_to_drop, drop=True):\n",
    "    if drop:\n",
    "        X = X.drop(columns=columns_to_drop, errors='ignore')\n",
    "    return X\n",
    "\n",
    "\n",
    "# custom function to drop rows with ones in y\n",
    "def drop_rows_with_ones(X, y=None):\n",
    "    if y is not None:\n",
    "        mask = y!=1\n",
    "        return X[mask], y[mask]\n",
    "    return X\n",
    "\n",
    "\n",
    "# custom function to drop rows with nans in either X or y\n",
    "def drop_rows_with_nans(X, y=None):\n",
    "    nan_mask_X = np.isnan(X).any(axis=1)\n",
    "    if y is not None:\n",
    "        nan_mask_y = np.isnan(y)\n",
    "        nan_mask = nan_mask_X | nan_mask_y\n",
    "        X_clean = X[~nan_mask]\n",
    "        y_clean = y[~nan_mask]\n",
    "        return X_clean, y_clean\n",
    "    else:\n",
    "        X_clean = X[~nan_mask_X]\n",
    "        return X_clean\n",
    "\n",
    "\n",
    "# function to create monotonic constraints for specific features\n",
    "def monotonic_constraints(num_vars, ord_vars, cat_vars, cat_encode, X, features_increasing, features_decreasing):\n",
    "    new_column_order = num_vars\n",
    "    new_column_order.extend(ord_vars)\n",
    "    if cat_encode != 'OHE':\n",
    "        new_column_order.extend(cat_vars)\n",
    "    else:\n",
    "        for col in cat_vars:\n",
    "            ohe = OneHotEncoder(sparse=False)\n",
    "            ohe.fit_transform(X[[col]])\n",
    "            new_column_order.extend(ohe.categories_[0])\n",
    "    monotone_constraints = []\n",
    "    for col in new_column_order:\n",
    "        if col in features_increasing:\n",
    "            monotone_constraints.append(1)\n",
    "        elif col in features_decreasing:\n",
    "            monotone_constraints.append(-1)\n",
    "        else:\n",
    "            monotone_constraints.append(0)\n",
    "    return monotone_constraints, new_column_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_settings import models\n",
    "\n",
    "model_for_mapie = 'XGB'\n",
    "# model_for_mapie = 'LGBM'\n",
    "\n",
    "print(f\"{model_for_mapie}: {models[model_for_mapie]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_sample_weights(y, q=(0.1,0.9), w=2.):\n",
    "    # get quantiles of the target variable\n",
    "    if isinstance(q,tuple):\n",
    "        qmin, qmax = np.max([0.,q[0]*100]), np.min([q[1]*100,100.])\n",
    "    else:\n",
    "        qmin, qmax = q*100, 100-(q*100)\n",
    "    target_quantiles = np.percentile(y, [qmin, qmax])\n",
    "    # assign weights based on quantiles\n",
    "    weights = np.zeros_like(y, dtype=float)\n",
    "    weights[y < target_quantiles[0]] = w  # lower quantile, assign higher weight\n",
    "    weights[(y >= target_quantiles[0]) & (y <= target_quantiles[1])] = 1.\n",
    "    weights[y > target_quantiles[1]] = w  # upper quantile, assign higher weight\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_sample_weights:\n",
    "    print(set_sample_weights(y_train, q=fit_quantiles_tails, w=fit_weight_tails))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_settings import target, locations_to_use, frequencies_to_use, columns_not_in_model\n",
    "from model_settings import imputation_method, scaler_type, scaler_quantile_nquantiles, scaler_rowwise_norm\n",
    "from model_settings import use_categorical_encoding, use_ordinal_encoding, use_numeric_transform, pca_transform_numeric, use_target_transform\n",
    "from model_settings import ordinal_columns, continuous_to_ordinal_transform_nbins, categorical_columns, categorical_encode_method, numeric_transform_method\n",
    "from model_settings import target_transform_method, target_yeojohnson_lambda, target_yeojohnson_nan_percentile\n",
    "from model_settings import use_sample_weights, fit_weight_tails, fit_quantiles_tails\n",
    "from model_settings import set_monotonic_constraints, features_monotone_increasing, features_monotone_decreasing, monotone_constraints_method_lgbm\n",
    "from model_settings import models\n",
    "\n",
    "# split ordinal features into discrete and continuous\n",
    "discrete_to_ordinal, continuous_to_ordinal = [], []\n",
    "for col in ordinal_columns:\n",
    "    if col not in columns_not_in_model:\n",
    "        if X_train[col].dtype in ['category', 'object']:\n",
    "            discrete_to_ordinal.append(col)\n",
    "        else:\n",
    "            continuous_to_ordinal.append(col)\n",
    "\n",
    "# convert X_train columns to list\n",
    "Xtrain_columns = X_train.columns.tolist()\n",
    "# set up amodel pipeline\n",
    "mdl_type = model_for_mapie\n",
    "rgr_mdl = copy.deepcopy(models[mdl_type].get('model', None))\n",
    "model = regression_model_pipeline()\n",
    "\n",
    "# split pipeline in preprocessor and model (works better for mapie 'aci' method)\n",
    "preprocessor = model.named_steps['preprocessor']\n",
    "model = model.named_steps['model']\n",
    "# fit preprocessor before model training\n",
    "preprocessor.fit(X_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preprocessor:')\n",
    "display(preprocessor)\n",
    "print('\\nModel:')\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get prediction intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enbpi_cv = False  # if False, EnBPI with BlockBootstrap is not used\n",
    "aci_cv = False  # if False, ACI with prefit model is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapie_alpha = .1\n",
    "aci_gamma = 1e-3\n",
    "print(f\"alpha = {mapie_alpha}, gamma = {aci_gamma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_for_conformal_cv = ['Europlatform']\n",
    "locations_for_conformal_prefit = ['Europlatform', 'Eurogeul E13', 'Eurogeul DWE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_int, cp_metrics = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if required, use 'prefit' model with interval fit based on calibration set to avoid memory errors when using mapie for large datasets\n",
    "# see: https://github.com/scikit-learn-contrib/MAPIE/issues/326"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for source code and implementation of MWIS metric (mean Winkler Interval score) see:  \n",
    "# https://www.kaggle.com/datasets/carlmcbrideellis/winkler-interval-score-metric  \n",
    "# https://www.kaggle.com/code/carlmcbrideellis/regression-prediction-intervals-with-mapie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set block bootstrap CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (enbpi_cv) | (aci_cv):\n",
    "    block_length = 50\n",
    "    n_resamplings = 20\n",
    "    max_test_size = 30000\n",
    "    reduce_test_size = False\n",
    "\n",
    "    if reduce_test_size:\n",
    "        cv_mapiets = BlockBootstrapWithTestSizeLimit(n_resamplings=n_resamplings, length=block_length, max_test_size=max_test_size, random_state=seed_value)\n",
    "    else:\n",
    "        cv_mapiets = BlockBootstrap(n_resamplings=n_resamplings, length=block_length, random_state=seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EnbPI conformal prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated in the MAPIE documentation (https://mapie.readthedocs.io/en/latest/generated/mapie.regression.MapieTimeSeriesRegressor.html), EnbPI only corresponds to MapieTimeSeriesRegressor if the cv argument is of type BlockBootstrap. Since BlockBootstrap with large sample size can produce a memory error, a custom class was created 'BlockBootstrapWithTestSizeLimit' to properly apply EnBPI with a timeseries based cv approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EnbPI generates prediction intervals using an ensemble of models and then applies conformal prediction to this ensemble. The idea is to use the ensemble for robust point predictions and then quantify uncertainty by calibrating the intervals based on past performance of the ensemble model. Instead of recalculating intervals at every time point like ACI, EnbPI typically processes data in batches (or windows). It uses the batch predictions from the ensemble model to construct the intervals. These batches can be overlapping or non-overlapping windows, depending on the design.EnbPI does not adapt as fluidly to every individual time step as ACI does but instead aggregates over a batch of predictions.  \n",
    "\n",
    "EnBPI typically requires block bootstrap or similar approaches as its cross-validation (CV) technique. This is because EnBPI is designed for batch processing and often leverages bootstrap resampling to capture variability in residuals over time. The block bootstrap approach also ensures that the time dependencies in the data (e.g., autocorrelation in time series) are preserved when estimating prediction intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EnBPI - block bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if enbpi_cv:\n",
    "    gap = 6\n",
    "\n",
    "    mdl_cv = copy.deepcopy(model)\n",
    "    mapie_enbpi = MapieTimeSeriesRegressor(mdl_cv, method=\"enbpi\", cv=cv_mapiets, agg_function=\"mean\")\n",
    "    print('Fit MAPIE TimeSeriesRegressor with EnBPI')\n",
    "    mapie_enbpi.fit(preprocessor.transform(X_train.sort_values(['Location', 'Frequency', 'Time'])), y_train.loc[X_train.sort_values(['Location', 'Frequency', 'Time']).index].to_numpy());\n",
    "\n",
    "    predint_dict = {}\n",
    "    coverage_dict = {}\n",
    "\n",
    "    for loc_name in locations_for_conformal_cv:\n",
    "        print(f\"Location: {loc_name}\")\n",
    "        for j,freq in enumerate(X_calib['Frequency'].unique()):\n",
    "            print(f\"  ({j+1}) Frequency: {freq}\")\n",
    "            mapie_enbpi_pfit = copy.deepcopy(mapie_enbpi)\n",
    "\n",
    "            Xnew_pfit = X_new[(X_new['Location'] == loc_name) & (X_new['Frequency'] == freq)]\n",
    "            ynew_pfit = y_new[(X_new['Location'] == loc_name) & (X_new['Frequency'] == freq)]\n",
    "            Xnew_tf = preprocessor.transform(Xnew_pfit)\n",
    "\n",
    "            y_pred_pfit, y_pis_pfit = np.zeros(len(Xnew_tf)), np.zeros((len(Xnew_tf),2, 1))\n",
    "            y_pred_pfit[:gap], y_pis_pfit[:gap, :, :] = mapie_enbpi_pfit.predict(\n",
    "                Xnew_tf[:gap, :], \n",
    "                alpha=mapie_alpha, \n",
    "                ensemble=True, \n",
    "                optimize_beta=True\n",
    "            )\n",
    "\n",
    "            for step in tqdm(range(gap, len(Xnew_pfit), gap)):\n",
    "                mapie_enbpi_pfit.partial_fit(\n",
    "                    Xnew_tf[(step - gap):step, :], \n",
    "                    ynew_pfit.iloc[(step - gap):step].to_numpy()\n",
    "                )\n",
    "                y_pred_pfit[step:step + gap], y_pis_pfit[step:step + gap, :, :] = mapie_enbpi_pfit.predict(\n",
    "                    Xnew_tf[step:(step + gap), :], \n",
    "                    alpha=mapie_alpha, \n",
    "                    ensemble=True, \n",
    "                    optimize_beta=True\n",
    "                )\n",
    "                y_pis_pfit[step:step + gap, :, :] = np.clip(\n",
    "                    y_pis_pfit[step:step + gap, :, :], -10, 10\n",
    "                )\n",
    "            predint_dict[f\"{loc_name.replace(' ', '-')}_{freq}\"] = {\n",
    "                'y':ynew_pfit.to_numpy(),\n",
    "                'yhat':y_pred_pfit,\n",
    "                'q_lower':y_pis_pfit[:,0].flatten(),\n",
    "                'q_upper':y_pis_pfit[:,1].flatten(),\n",
    "                'interval':y_pis_pfit[:,1].flatten() - y_pis_pfit[:,0].flatten(),\n",
    "                'error':y_pred_pfit - ynew_pfit.to_numpy(),\n",
    "                'freq':Xnew_pfit['Frequency'].values,\n",
    "                'time':Xnew_pfit['Time'].values,\n",
    "                'location':Xnew_pfit['Location'].values,\n",
    "                'index':Xnew_pfit.index.values,\n",
    "            }\n",
    "            coverage_dict[f\"{loc_name.replace(' ', '-')}_{freq}\"] = {\n",
    "                'coverage':regression_coverage_score(ynew_pfit.to_numpy(), y_pis_pfit[:, 0, 0], y_pis_pfit[:, 1, 0]),\n",
    "                'width':regression_mean_width_score(y_pis_pfit[:, 0, 0], y_pis_pfit[:, 1, 0]),\n",
    "                'cwc':coverage_width_based(ynew_pfit.to_numpy(), y_pis_pfit[:, 0, 0], y_pis_pfit[:, 1, 0], eta=10, alpha=mapie_alpha),\n",
    "                'winkler':MWIS_metric.score(ynew_pfit.to_numpy(), y_pis_pfit[:, 0, 0], y_pis_pfit[:, 1, 0], alpha=mapie_alpha)[0],\n",
    "                'freq':freq,\n",
    "                'location':loc_name,            \n",
    "            }\n",
    "        print('')\n",
    "\n",
    "    pred_int['enbpi'] = pd.DataFrame()\n",
    "    for key in predint_dict.keys():\n",
    "        pred_int['enbpi'] = pd.concat([pred_int['enbpi'], pd.DataFrame(predint_dict[key])], axis='index', ignore_index=True)\n",
    "    pred_int['enbpi'] = pred_int['aci'].set_index('index')\n",
    "    pred_int['enbpi'].index.name = None\n",
    "\n",
    "    cp_metrics['enbpi'] = pd.DataFrame()\n",
    "    for key in predint_dict.keys():\n",
    "        cp_metrics['enbpi'] = pd.concat([cp_metrics['enbpi'], pd.DataFrame.from_dict(coverage_dict[key], orient='index').transpose()], axis='index', ignore_index=True)\n",
    "    cp_metrics['enbpi'] = cp_metrics['enbpi'].sort_values(['location','freq']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACI conformal prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACI updates the width of prediction intervals dynamically, based on the distribution of past residuals (errors in the model’s previous predictions). This approach uses historical data to adjust its intervals over time in an adaptive way, which is important in time series, as the underlying data distribution may change (non-stationarity). The key feature of ACI is that it adapts to changes in the distribution of errors. If the time series becomes more volatile (with larger residuals) or experiences a sudden shift, ACI will produce wider intervals to maintain coverage. Conversely, in more stable periods, it will reduce the width of the intervals.  \n",
    "\n",
    "ACI does not necessarily require cross-validation or block bootstrap like EnBPI does. Instead, ACI can be applied with either a prefit model or cross-validation. With a prefit model ACI computes prediction intervals sequentially, based on the residuals of the pre-fit model's predictions over time. There’s no need to refit the model during the conformal inference process, and as long as the residuals are updated continuously, the prediction intervals remain valid. ACI can also work with cross-validation to improve generalization but this is not a strict requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACI - block bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if aci_cv:\n",
    "    gap = 6\n",
    "    \n",
    "    mdl_cv = copy.deepcopy(model)\n",
    "    mapie_aci = MapieTimeSeriesRegressor(mdl_cv, method=\"aci\", cv=cv_mapiets, agg_function=\"mean\")\n",
    "    print('Fit MAPIE TimeSeriesRegressor with ACI')\n",
    "    mapie_aci.fit(preprocessor.transform(X_train.sort_values(['Location', 'Frequency', 'Time'])), y_train.loc[X_train.sort_values(['Location', 'Frequency', 'Time']).index].to_numpy())\n",
    "\n",
    "    predint_dict = {}\n",
    "    coverage_dict = {}\n",
    "\n",
    "    for loc_name in locations_for_conformal_prediction:\n",
    "        print(f\"Location: {loc_name}\")\n",
    "        for j,freq in enumerate(X_calib['Frequency'].unique()):\n",
    "            print(f\"  ({j+1}) Frequency: {freq}\")\n",
    "            mapie_aci_pfit = copy.deepcopy(mapie_aci)\n",
    "\n",
    "            Xnew_pfit = X_new[(X_new['Location'] == loc_name) & (X_new['Frequency'] == freq)]\n",
    "            ynew_pfit = y_new[(X_new['Location'] == loc_name) & (X_new['Frequency'] == freq)]\n",
    "            Xnew_tf = preprocessor.transform(Xnew_pfit)\n",
    "\n",
    "            y_pred_pfit, y_pis_pfit = np.zeros(len(Xnew_tf)), np.zeros((len(Xnew_tf),2, 1))\n",
    "            y_pred_pfit[:gap], y_pis_pfit[:gap, :, :] = mapie_aci_pfit.predict(\n",
    "                Xnew_tf[:gap, :], \n",
    "                alpha=mapie_alpha, \n",
    "                ensemble=True, \n",
    "                optimize_beta=False,\n",
    "                allow_infinite_bounds=True,\n",
    "            )\n",
    "\n",
    "            for step in tqdm(range(gap, len(Xnew_pfit), gap)):\n",
    "                mapie_aci_pfit.partial_fit(\n",
    "                    Xnew_tf[(step - gap):step, :], \n",
    "                    ynew_pfit.iloc[(step - gap):step].to_numpy()\n",
    "                )\n",
    "                mapie_aci_pfit.adapt_conformal_inference(\n",
    "                    Xnew_tf[(step - gap):step, :], \n",
    "                    ynew_pfit.iloc[(step - gap):step].to_numpy(),\n",
    "                    gamma=aci_gamma\n",
    "                )\n",
    "                y_pred_pfit[step:step + gap], y_pis_pfit[step:step + gap, :, :] = mapie_aci_pfit.predict(\n",
    "                    Xnew_tf[step:(step + gap), :], \n",
    "                    alpha=mapie_alpha, \n",
    "                    ensemble=True, \n",
    "                    optimize_beta=False,\n",
    "                    allow_infinite_bounds=True\n",
    "                )\n",
    "                y_pis_pfit[step:step + gap, :, :] = np.clip(\n",
    "                    y_pis_pfit[step:step + gap, :, :], -10, 10\n",
    "                )\n",
    "            predint_dict[f\"{loc_name.replace(' ', '-')}_{freq}\"] = {\n",
    "                'y':ynew_pfit.to_numpy(),\n",
    "                'yhat':y_pred_pfit,\n",
    "                'q_lower':y_pis_pfit[:,0].flatten(),\n",
    "                'q_upper':y_pis_pfit[:,1].flatten(),\n",
    "                'interval':y_pis_pfit[:,1].flatten() - y_pis_pfit[:,0].flatten(),\n",
    "                'error':y_pred_pfit - ynew_pfit.to_numpy(),\n",
    "                'freq':Xnew_pfit['Frequency'].values,\n",
    "                'time':Xnew_pfit['Time'].values,\n",
    "                'location':Xnew_pfit['Location'].values,\n",
    "                'index':Xnew_pfit.index.values,\n",
    "            }\n",
    "            coverage_dict[f\"{loc_name.replace(' ', '-')}_{freq}\"] = {\n",
    "                'coverage':regression_coverage_score(ynew_pfit.to_numpy(), y_pis_pfit[:, 0, 0], y_pis_pfit[:, 1, 0]),\n",
    "                'width':regression_mean_width_score(y_pis_pfit[:, 0, 0], y_pis_pfit[:, 1, 0]),\n",
    "                'cwc':coverage_width_based(ynew_pfit.to_numpy(), y_pis_pfit[:, 0, 0], y_pis_pfit[:, 1, 0], eta=10, alpha=mapie_alpha),\n",
    "                'winkler':MWIS_metric.score(ynew_pfit.to_numpy(), y_pis_pfit[:, 0, 0], y_pis_pfit[:, 1, 0], alpha=mapie_alpha)[0],\n",
    "                'freq':freq,\n",
    "                'location':loc_name,            \n",
    "            }\n",
    "        print('')\n",
    "\n",
    "    pred_int['aci'] = pd.DataFrame()\n",
    "    for key in predint_dict.keys():\n",
    "        pred_int['aci'] = pd.concat([pred_int['aci'], pd.DataFrame(predint_dict[key])], axis='index', ignore_index=True)\n",
    "    pred_int['aci'] = pred_int['aci'].set_index('index')\n",
    "    pred_int['aci'].index.name = None\n",
    "\n",
    "    cp_metrics['aci'] = pd.DataFrame()\n",
    "    for key in predint_dict.keys():\n",
    "        cp_metrics['aci'] = pd.concat([cp_metrics['aci'], pd.DataFrame.from_dict(coverage_dict[key], orient='index').transpose()], axis='index', ignore_index=True)\n",
    "    cp_metrics['aci'] = cp_metrics['aci'].sort_values(['location','freq']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACI - prefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not aci_cv:\n",
    "    gap = 1\n",
    "\n",
    "    # prefit model\n",
    "    mdl_prefit = copy.deepcopy(model)\n",
    "    if use_sample_weights:\n",
    "        model_hasattr_sample_weight = True\n",
    "        try:    \n",
    "            mdl_prefit.fit(preprocessor.transform(X_train), y_train.to_numpy(), model__sample_weight=np.ones(len(y_train)))\n",
    "        except:\n",
    "            model_hasattr_sample_weight = False\n",
    "\n",
    "        if model_hasattr_sample_weight:\n",
    "            print('   Applying sample weights for model training')\n",
    "            mdl_prefit.fit(preprocessor.transform(X_train), y_train.to_numpy(), model__sample_weight=set_sample_weights(y_train, q=fit_quantiles_tails, w=fit_weight_tails))\n",
    "        else:\n",
    "            mdl_prefit.fit(preprocessor.transform(X_train), y_train.to_numpy())\n",
    "    else:\n",
    "        mdl_prefit.fit(preprocessor.transform(X_train), y_train.to_numpy())\n",
    "\n",
    "    mapie_aci = MapieTimeSeriesRegressor(mdl_prefit, method=\"aci\", cv=\"prefit\")\n",
    "\n",
    "    predint_dict = {}\n",
    "    coverage_dict = {}\n",
    "\n",
    "    for loc_name in locations_for_conformal_prefit:\n",
    "        print(f\"Location: {loc_name}\")\n",
    "        for j,freq in enumerate(X_calib['Frequency'].unique()):\n",
    "            print(f\"   ({j+1}/{len(X_calib['Frequency'].unique())}) Frequency: {freq}\")\n",
    "            Xcalib_tf = preprocessor.transform(X_calib[(X_calib['Location'] == loc_name) & (X_calib['Frequency'] == freq)])\n",
    "            mapie_aci_pfit = copy.deepcopy(mapie_aci).fit(Xcalib_tf, y_calib[(X_calib['Location'] == loc_name) & (X_calib['Frequency'] == freq)].to_numpy())\n",
    "\n",
    "            Xnew_pfit = X_new[(X_new['Location'] == loc_name) & (X_new['Frequency'] == freq)]\n",
    "            ynew_pfit = y_new[(X_new['Location'] == loc_name) & (X_new['Frequency'] == freq)]\n",
    "            Xnew_tf = preprocessor.transform(Xnew_pfit)\n",
    "\n",
    "            y_pred_pfit, y_pis_pfit = np.zeros(len(Xnew_tf)), np.zeros((len(Xnew_tf),2, 1))\n",
    "            y_pred_pfit[:gap], y_pis_pfit[:gap, :, :] = mapie_aci_pfit.predict(\n",
    "                Xnew_tf[:gap, :], \n",
    "                alpha=mapie_alpha, \n",
    "                ensemble=False, \n",
    "                optimize_beta=False,\n",
    "                allow_infinite_bounds=True\n",
    "            )\n",
    "            for step in tqdm(range(gap, len(Xnew_pfit), gap)):\n",
    "                mapie_aci_pfit.partial_fit(\n",
    "                    Xnew_tf[(step - gap):step, :], \n",
    "                    ynew_pfit.iloc[(step - gap):step].to_numpy()\n",
    "                )\n",
    "                mapie_aci_pfit.adapt_conformal_inference(\n",
    "                    Xnew_tf[(step - gap):step, :], \n",
    "                    ynew_pfit.iloc[(step - gap):step].to_numpy(),\n",
    "                    gamma=aci_gamma\n",
    "                )\n",
    "                y_pred_pfit[step:step + gap], y_pis_pfit[step:step + gap, :, :] = mapie_aci_pfit.predict(\n",
    "                    Xnew_tf[step:(step + gap), :], \n",
    "                    alpha=mapie_alpha, \n",
    "                    ensemble=False, \n",
    "                    optimize_beta=False,\n",
    "                    allow_infinite_bounds=True\n",
    "                )\n",
    "                y_pis_pfit[step:step + gap, :, :] = np.clip(\n",
    "                    y_pis_pfit[step:step + gap, :, :], -10, 10\n",
    "                )\n",
    "            predint_dict[f\"{loc_name.replace(' ', '-')}_{freq}\"] = {\n",
    "                'y':ynew_pfit.to_numpy(),\n",
    "                'yhat':y_pred_pfit,\n",
    "                'q_lower':y_pis_pfit[:,0].flatten(),\n",
    "                'q_upper':y_pis_pfit[:,1].flatten(),\n",
    "                'interval':y_pis_pfit[:,1].flatten() - y_pis_pfit[:,0].flatten(),\n",
    "                'error':y_pred_pfit - ynew_pfit.to_numpy(),\n",
    "                'freq':Xnew_pfit['Frequency'].values,\n",
    "                'time':Xnew_pfit['Time'].values,\n",
    "                'location':Xnew_pfit['Location'].values,\n",
    "                'index':Xnew_pfit.index.values,\n",
    "            }\n",
    "            coverage_dict[f\"{loc_name.replace(' ', '-')}_{freq}\"] = {\n",
    "                'coverage':regression_coverage_score(ynew_pfit.to_numpy(), y_pis_pfit[:, 0, 0], y_pis_pfit[:, 1, 0]),\n",
    "                'width':regression_mean_width_score(y_pis_pfit[:, 0, 0], y_pis_pfit[:, 1, 0]),\n",
    "                'cwc':coverage_width_based(ynew_pfit.to_numpy(), y_pis_pfit[:, 0, 0], y_pis_pfit[:, 1, 0], eta=10, alpha=mapie_alpha),\n",
    "                'winkler':MWIS_metric.score(ynew_pfit.to_numpy(), y_pis_pfit[:, 0, 0], y_pis_pfit[:, 1, 0], alpha=mapie_alpha)[0],\n",
    "                'freq':freq,\n",
    "                'location':loc_name,            \n",
    "            }\n",
    "        print('')\n",
    "\n",
    "    pred_int['aci'] = pd.DataFrame()\n",
    "    for key in predint_dict.keys():\n",
    "        pred_int['aci'] = pd.concat([pred_int['aci'], pd.DataFrame(predint_dict[key])], axis='index', ignore_index=True)\n",
    "    pred_int['aci'] = pred_int['aci'].set_index('index')\n",
    "    pred_int['aci'].index.name = None\n",
    "\n",
    "    cp_metrics['aci'] = pd.DataFrame()\n",
    "    for key in predint_dict.keys():\n",
    "        cp_metrics['aci'] = pd.concat([cp_metrics['aci'], pd.DataFrame.from_dict(coverage_dict[key], orient='index').transpose()], axis='index', ignore_index=True)\n",
    "    cp_metrics['aci'] = cp_metrics['aci'].sort_values(['location','freq']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CP results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'enbpi' in pred_int.keys():\n",
    "    display(pred_int['enbpi'].tail(5))\n",
    "    display(cp_metrics['enbpi'].tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'aci' in pred_int.keys():\n",
    "    display(pred_int['aci'].tail(5))\n",
    "    display(cp_metrics['aci'].tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot conformal metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_names = {\n",
    "    'coverage':'Effective coverage',\n",
    "    'width':'Effective mean width',\n",
    "    'winkler':'Winkler Interval score',\n",
    "}\n",
    "location_colors = {\n",
    "    'Europlatform':'tab:blue',\n",
    "    'Eurogeul E13':'tab:orange',\n",
    "    'Eurogeul DWE':'tab:green',\n",
    "}\n",
    "\n",
    "for cp_method in cp_metrics.keys():\n",
    "    fig, axes = plt.subplots(figsize=(8,4.5), nrows=3, ncols=1, sharex=True)\n",
    "    for j,col in enumerate(['width','coverage','winkler']):\n",
    "        ax = axes.flatten()[j]\n",
    "        ax.set_title(f\"{metrics_names[col]} ({cp_method.replace('enbpi','EnbPI').replace('aci','ACI')})\")\n",
    "        ax.grid(axis='both', color=[.7,.7,.7], linestyle='-', linewidth=.5)\n",
    "        ax.set_axisbelow(True)\n",
    "        for loc_name in location_colors.keys():\n",
    "            if loc_name in cp_metrics[cp_method]['location'].unique():\n",
    "                ax.plot(\n",
    "                    cp_metrics[cp_method].loc[cp_metrics[cp_method]['location'] == loc_name, 'freq'], \n",
    "                    cp_metrics[cp_method].loc[cp_metrics[cp_method]['location'] == loc_name, col],\n",
    "                    lw=1., marker='o', markersize=2, color=location_colors[loc_name]\n",
    "                )\n",
    "\n",
    "    legend_patches = {}\n",
    "    for loc_name in location_colors.keys():\n",
    "        if loc_name in cp_metrics[cp_method]['location'].unique():\n",
    "            legend_patches[loc_name] = mpatches.Patch(facecolor=location_colors[loc_name], alpha=1., edgecolor='gray', linewidth=.1)\n",
    "    fig.legend(handles=list(legend_patches.values()), labels=list(legend_patches.keys()), fontsize=10, bbox_to_anchor=(0.98,0.935), loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot prediction intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_intervals(ax, intervals, freq, location, target, alpha, cp_type, mdl, x_col='rank', sort_by='y', data_to_plot='values', sample_size=1., smoothing_window=None, is_quantile=False, xlim_fixed=None, ylim_fixed=None, signed_logy=False, show_observations=True, show_outliers=False, show_axistitle=True, markersize=5.5):\n",
    "    plot_df = intervals.copy()\n",
    "    plot_df = plot_df.loc[(plot_df['freq']==freq) & (plot_df['location']==location)]\n",
    "    plot_df['mean_interval'] = plot_df['interval'].mean()\n",
    "    if smoothing_window is not None:\n",
    "        plot_df['interval'] = plot_df['interval'].rolling(smoothing_window, center=True).mean()\n",
    "        plot_df['q_lower'] = plot_df['q_lower'].rolling(smoothing_window, center=True).mean()\n",
    "        plot_df['q_upper'] = plot_df['q_upper'].rolling(smoothing_window, center=True).mean()\n",
    "    plot_df['outside_interval'] = (plot_df['y'] < plot_df['q_lower']) | (plot_df['y'] > plot_df['q_upper'])\n",
    "    if signed_logy:\n",
    "        plot_df['y'] = np.sign(plot_df['y']) * np.log1p(np.abs(plot_df['y']))\n",
    "        plot_df['yhat'] = np.sign(plot_df['yhat']) * np.log1p(np.abs(plot_df['yhat']))\n",
    "        plot_df['q_lower'] = np.sign(plot_df['q_lower']) * np.log1p(np.abs(plot_df['q_lower']))\n",
    "        plot_df['q_upper'] = np.sign(plot_df['q_upper']) * np.log1p(np.abs(plot_df['q_upper']))\n",
    "        plot_df['interval'] = np.sign(plot_df['interval']) * np.log1p(np.abs(plot_df['interval']))\n",
    "        plot_df['mean_interval'] = np.sign(plot_df['mean_interval']) * np.log1p(np.abs(plot_df['mean_interval']))\n",
    "    if x_col == 'time':\n",
    "        plot_df = plot_df.sort_values('time')\n",
    "    elif sort_by is not None:\n",
    "        plot_df = plot_df.sort_values(sort_by)\n",
    "    plot_df['rank'] = np.cumsum(np.ones(shape=(plot_df.shape[0]),dtype=int))\n",
    "    plot_df = plot_df.reset_index().sample(n=int(sample_size*plot_df.shape[0])).set_index('index').sort_values('rank')\n",
    "    if data_to_plot == 'values':\n",
    "        if show_observations:\n",
    "            ax.scatter(plot_df[x_col], plot_df['y'], edgecolor='darkorange', facecolor='darkorange', marker='s', s=markersize, lw=.7, label='Observed', zorder=10)\n",
    "            if show_outliers:\n",
    "                ax.scatter(\n",
    "                    plot_df.loc[plot_df['outside_interval'],x_col], plot_df.loc[plot_df['outside_interval'],'y'], \n",
    "                    edgecolor='firebrick', facecolor='firebrick', marker='s', s=markersize+1.5, lw=.7, label='Observations outside interval', zorder=30\n",
    "                )\n",
    "        ax.scatter(plot_df[x_col], plot_df['yhat'], edgecolor='royalblue', facecolor='royalblue', s=markersize, lw=.7, label='Predicted', zorder=20)\n",
    "        ax.plot(plot_df[x_col], plot_df['q_lower'], c='gray', ls='-', lw=1., zorder=0)\n",
    "        ax.plot(plot_df[x_col], plot_df['q_upper'], c='gray', ls='-', lw=1., zorder=0)\n",
    "        ax.fill_between(plot_df[x_col], plot_df['q_lower'], plot_df['q_upper'], color='gray', label='Prediction interval', alpha=0.3)\n",
    "    elif data_to_plot == 'errors':\n",
    "        if x_col == 'rank':\n",
    "            ax.plot([-20,intervals.shape[0]+20], [0,0], ls='-.', c='firebrick', lw=.7, zorder=20)\n",
    "        ax.scatter(plot_df[x_col], plot_df['error'], c='royalblue', s=markersize, label='Predicted - Observed', zorder=10)\n",
    "        ax.plot(plot_df[x_col], plot_df['q_lower'] - plot_df['y'], c='darkorange', ls='--', zorder=0)\n",
    "        ax.plot(plot_df[x_col], plot_df['q_upper'] - plot_df['y'], c='darkorange', ls='--', zorder=0)\n",
    "        ax.fill_between(plot_df[x_col], plot_df['q_lower'] - plot_df['y'], plot_df['q_upper'] - plot_df['y'], color='darkorange', label='Prediction interval', alpha=0.4)\n",
    "    elif data_to_plot == 'intervals':\n",
    "        ax.plot(plot_df[x_col], plot_df['interval'], c='royalblue', ls='-', lw=1.2, label='Prediction interval', zorder=10)\n",
    "        xvals = ax.get_xlim()\n",
    "        ymean = plot_df['mean_interval'].values[0]\n",
    "        ax.plot([xvals[0],xvals[1]], [ymean,ymean], c='black', ls='--', lw=.7, label='Average interval', zorder=0)\n",
    "        ax.set_xlim(xvals)\n",
    "    if x_col == 'time':\n",
    "        ax.set_xlabel('')\n",
    "    else:\n",
    "        ax.set_xlabel(x_col.capitalize())\n",
    "    if data_to_plot == 'values':\n",
    "        if signed_logy:\n",
    "            ax.set_ylabel(f\"{target} (signed log)\")\n",
    "        else:\n",
    "            ax.set_ylabel(target)\n",
    "    elif data_to_plot == 'errors':\n",
    "        ax.set_ylabel(f\"Predicted ({target}) - Observed ({target})\")\n",
    "    elif data_to_plot == 'intervals':\n",
    "        ax.set_ylabel(f\"Prediction interval ({int(np.round(100*(1-alpha),0))}% coverage)\")\n",
    "    cp_type = cp_type.replace('enbpi','EnbPI').replace('aci','ACI').replace('cqr','Quantile regression')\n",
    "    if show_axistitle:\n",
    "        if is_quantile: \n",
    "            ax.set_title(f\"Location: {location}, Frequency: {freq} - Prediction intervals ({int(np.round(100*(1-alpha),0))}% coverage, {cp_type})\\n{str(mdl).split('(')[0]} (conformalized quantile regression)\")\n",
    "        else:\n",
    "            ax.set_title(f\"Location: {location}, Frequency: {freq} - Prediction intervals ({int(np.round(100*(1-alpha),0))}% coverage, {cp_type})\\n{str(mdl).split('(')[0]} (conformalized regression)\")\n",
    "    ax.legend(bbox_to_anchor=(1,1.02), loc='upper left')\n",
    "    if ylim_fixed is not None:\n",
    "        ax.set_ylim(ylim_fixed)\n",
    "    elif data_to_plot == 'intervals':\n",
    "        if plot_df['interval'].max() - plot_df['interval'].min() < 1e-15:\n",
    "            ax.set_ylim(plot_df['interval'].median()*0.98, plot_df['interval'].median()*1.02)\n",
    "    \n",
    "    if xlim_fixed is not None:\n",
    "        ax.set_xlim(xlim_fixed)\n",
    "    ax.grid(axis='both', color=[.7,.7,.7], linestyle='-', linewidth=.5)\n",
    "    ax.set_axisbelow(True)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_idx = 16\n",
    "loc_idx = 2\n",
    "xlims_plot = pd.to_datetime(['2020-11-05', '2020-11-12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_method = 'aci'\n",
    "smoothing_window = 12\n",
    "\n",
    "freq_to_plot = X['Frequency'].unique()[freq_idx]\n",
    "if aci_cv:\n",
    "    loc_to_plot = locations_for_conformal_cv[0]\n",
    "else:\n",
    "    loc_to_plot = locations_for_conformal_prefit[loc_idx]\n",
    "if split_method == 'leave_one_out':\n",
    "    loc_to_plot = leave_one_out_location\n",
    "\n",
    "if cp_method in pred_int.keys():\n",
    "    fig, ax = plt.subplots(figsize=(16,7), nrows=2, ncols=1, sharex=True, dpi=150)\n",
    "\n",
    "    ax[0] = plot_prediction_intervals(\n",
    "        ax[0],\n",
    "        pred_int[cp_method],\n",
    "        freq_to_plot,\n",
    "        loc_to_plot,\n",
    "        'Correction of SWAN model result', \n",
    "        mapie_alpha,\n",
    "        cp_method,\n",
    "        mdl=models[mdl_type].get('name'), \n",
    "        x_col='time',  # 'rank' or 'time'\n",
    "        data_to_plot='values', \n",
    "        sort_by='time',\n",
    "        sample_size=1., \n",
    "        smoothing_window=smoothing_window, \n",
    "        # ylim_fixed=(-.5,1.5),\n",
    "        # xlim_fixed=xlims_plot,\n",
    "        # signed_logy=True,\n",
    "        show_observations=True,\n",
    "        show_outliers=True,\n",
    "        markersize=10,\n",
    "    )\n",
    "    ax[1] = plot_prediction_intervals(\n",
    "        ax[1],\n",
    "        pred_int[cp_method],\n",
    "        freq_to_plot,\n",
    "        loc_to_plot,\n",
    "        'Correction of SWAN model result', \n",
    "        mapie_alpha,\n",
    "        cp_method,\n",
    "        mdl=models[mdl_type].get('name'), \n",
    "        x_col='time',  # 'rank' or 'time'\n",
    "        data_to_plot='intervals', \n",
    "        sort_by='time',\n",
    "        sample_size=1.,\n",
    "        smoothing_window=smoothing_window, \n",
    "        # ylim_fixed=(0, 2),\n",
    "        # xlim_fixed=xlims_plot,\n",
    "        # signed_logy=True,\n",
    "        show_observations=True,\n",
    "        show_axistitle=False,\n",
    "    )\n",
    "    ax[0].tick_params(axis='x', labeltop=False, labelbottom=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_method = 'enbpi'\n",
    "\n",
    "freq_to_plot = X['Frequency'].unique()[freq_idx]\n",
    "loc_to_plot = locations_for_conformal_cv[0]\n",
    "if split_method == 'leave_one_out':\n",
    "    loc_to_plot = leave_one_out_location\n",
    "\n",
    "if cp_method in pred_int.keys():\n",
    "    fig, ax = plt.subplots(figsize=(16,7), nrows=2, ncols=1, sharex=True, dpi=150)\n",
    "\n",
    "    ax[0] = plot_prediction_intervals(\n",
    "        ax[0],\n",
    "        pred_int[cp_method],\n",
    "        freq_to_plot,\n",
    "        loc_to_plot,\n",
    "        'Correction of SWAN model result', \n",
    "        mapie_alpha,\n",
    "        cp_method,\n",
    "        mdl=models[mdl_type].get('name'), \n",
    "        x_col='time',  # 'rank' or 'time'\n",
    "        data_to_plot='values', \n",
    "        sort_by='time',\n",
    "        sample_size=1., \n",
    "        smoothing_window=None, \n",
    "        # ylim_fixed=(-.5,1.5),\n",
    "        # xlim_fixed=xlims_plot,\n",
    "        # signed_logy=True,\n",
    "        show_observations=True,\n",
    "        show_outliers=True,\n",
    "        markersize=10,\n",
    "    )\n",
    "    ax[1] = plot_prediction_intervals(\n",
    "        ax[1],\n",
    "        pred_int[cp_method],\n",
    "        freq_to_plot,\n",
    "        loc_to_plot,\n",
    "        'Correction of SWAN model result', \n",
    "        mapie_alpha,\n",
    "        cp_method,\n",
    "        mdl=models[mdl_type].get('name'), \n",
    "        x_col='time',  # 'rank' or 'time'\n",
    "        data_to_plot='intervals', \n",
    "        sort_by='time',\n",
    "        sample_size=1.,\n",
    "        smoothing_window=None, \n",
    "        # ylim_fixed=(0, 2),\n",
    "        # xlim_fixed=xlims_plot,\n",
    "        # signed_logy=True,\n",
    "        show_observations=True,\n",
    "        show_axistitle=False,\n",
    "    )\n",
    "    ax[0].tick_params(axis='x', labeltop=False, labelbottom=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get feature contribution to prediction intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_kernel_explainer(model, X_trn, y_trn, X_tst, y_tst, sample_method='random', use_testdata=False, sample_size=1000, background_size=20, background_method = 'random', k_kmeans=None, random_seed=42):\n",
    "    \n",
    "    # helper function to fix that kernel shap sends data as numpy array without column names\n",
    "    # see: https://datascience.stackexchange.com/questions/52476/how-to-use-shap-kernal-explainer-with-pipeline-models\n",
    "    def model_predict(data_asarray):\n",
    "        data_asframe = pd.DataFrame(data_asarray, columns=fnames)\n",
    "        return model.predict(data_asframe)\n",
    "    \n",
    "    \n",
    "    # Use test or train data for sample dataset\n",
    "    if use_testdata:\n",
    "        X_smp = X_tst.copy()\n",
    "        y_smp = y_tst.copy()\n",
    "    else:\n",
    "        X_smp = X_trn.copy()\n",
    "        y_smp = y_trn.copy()\n",
    "    # Use train data as background dataset\n",
    "    X_bg = X_trn.copy()\n",
    "\n",
    "    # Take sample from dataset\n",
    "    if (sample_size is not None):\n",
    "         if sample_size < X_smp.shape[0]:\n",
    "            if sample_method == 'random':\n",
    "                X_smp, _, y_smp, _ = train_test_split(X_smp, y_smp, train_size=min([sample_size, X_smp.shape[0]]), random_state=random_seed)\n",
    "            elif sample_method == 'stratified':\n",
    "                groups = create_bins(y_smp, num_bins=10, use_log=True)\n",
    "                X_smp, _, y_smp, _ = train_test_split(X_smp, y_smp, train_size=min([sample_size, X_smp.shape[0]]), stratify=groups, random_state=random_seed)\n",
    "\n",
    "    # Take sample from background set\n",
    "    print(f\"   Take sample from background set ({background_method})\")\n",
    "    if background_method == 'kmeans':\n",
    "        X_bg = shap.kmeans(X_bg, k=k_kmeans)\n",
    "    else:\n",
    "        X_bg = X_bg.sample(n=min([background_size, X_bg.shape[0]]), random_state=random_seed).values\n",
    "\n",
    "    # Explain predicted values\n",
    "    print('   Create explainer')\n",
    "    fnames = X_smp.columns.tolist()\n",
    "    explainer = shap.KernelExplainer(\n",
    "        model_predict, \n",
    "        X_bg,\n",
    "    )\n",
    "    # Get SHAP values\n",
    "    print('   Get SHAP values')\n",
    "    shap_values = explainer.shap_values(X_smp)\n",
    "\n",
    "    # Build results dictionary\n",
    "    shap_res = {}\n",
    "    shap_res['explainer'] = explainer\n",
    "    shap_res['shap_values'] = shap_values\n",
    "    shap_res['shap_feature_names'] = fnames\n",
    "    shap_res['X'] = X_smp\n",
    "    shap_res['y'] = y_smp\n",
    "    shap_res['X_transformed'] = X_smp\n",
    "    shap_res['y_pred'] = pd.Series(model.predict(X_smp), index=y_smp.index)\n",
    "    \n",
    "    return shap_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_permutation_explainer(model, X_trn, y_trn, X_tst, y_tst, sample_method='random', use_testdata=False, sample_size=1000, background_size=20, background_method = 'random', k_kmeans=5, ord_feats=[], cat_feats=[], drop_feats=[], random_seed=42):\n",
    "    \n",
    "    # define 'column' order in np.array after preprocessing (1. numeric, 2. ordinal, 3. categorical)\n",
    "    ord_var_names = X_trn.columns[(X_trn.columns.isin(ord_feats)) & (~X_trn.columns.isin(drop_feats))].tolist()\n",
    "    cat_var_names = X_trn.columns[(X_trn.columns.isin(cat_feats)) & (~X_trn.columns.isin(drop_feats))].tolist()\n",
    "    num_var_names = X_trn.columns[(~X_trn.columns.isin(cat_var_names)) & (~X_trn.columns.isin(ord_var_names)) & (~X_trn.columns.isin(drop_feats))].tolist()\n",
    "    new_column_order = num_var_names\n",
    "    new_column_order.extend(ord_var_names)\n",
    "    new_column_order.extend(cat_var_names)\n",
    "\n",
    "    # Use test or train data for sample dataset\n",
    "    if use_testdata:\n",
    "        X_smp = X_tst.copy()\n",
    "        y_smp = y_tst.copy()\n",
    "    else:\n",
    "        X_smp = X_trn.copy()\n",
    "        y_smp = y_trn.copy()\n",
    "    # Use train data as background dataset\n",
    "    X_bg = X_trn.copy()\n",
    "    y_bg = y_trn.copy()\n",
    "\n",
    "    # print(X_smp.shape[0])\n",
    "    \n",
    "    # Take sample from dataset\n",
    "    if (sample_size is not None):\n",
    "         if sample_size < X_smp.shape[0]:\n",
    "            if sample_method == 'random':\n",
    "                X_smp, _, y_smp, _ = train_test_split(X_smp, y_smp, train_size=min([sample_size, X_smp.shape[0]]), random_state=random_seed)\n",
    "            elif sample_method == 'stratified':\n",
    "                groups = create_bins(y_smp, num_bins=10, use_log=True)\n",
    "                X_smp, _, y_smp, _ = train_test_split(X_smp, y_smp, train_size=min([sample_size, X_smp.shape[0]]), stratify=groups, random_state=random_seed)\n",
    "    \n",
    "    # Transform features\n",
    "    if 'preprocessor' in model.named_steps:\n",
    "        data_transformer = model.named_steps['preprocessor'].fit(X_trn, y_trn)\n",
    "        transformer_uses_y = True\n",
    "        try:\n",
    "            data_transformer.transform(X_smp, y_smp)\n",
    "        except:\n",
    "            transformer_uses_y = False\n",
    "        if transformer_uses_y:\n",
    "            X_tf = data_transformer.transform(X_smp, y_smp)\n",
    "        else:\n",
    "            X_tf = data_transformer.transform(X_smp)\n",
    "    else:\n",
    "        X_tf = X_smp.values\n",
    "\n",
    "    # Make background dataset\n",
    "    if background_size is not None:\n",
    "        X_bg = X_bg.sample(n=min([background_size, X_bg.shape[0]]), random_state=random_seed)\n",
    "        y_bg = y_bg.sample(n=min([background_size, y_bg.shape[0]]), random_state=random_seed)\n",
    "    if 'preprocessor' in model.named_steps:\n",
    "        if transformer_uses_y:\n",
    "            X_bg = data_transformer.transform(X_bg, y_bg)\n",
    "        else:\n",
    "            X_bg = data_transformer.transform(X_bg)\n",
    "\n",
    "    # Explain predicted values\n",
    "    print('   Create explainer')\n",
    "    explainer = shap.PermutationExplainer(\n",
    "        model.named_steps['model'].predict, \n",
    "        X_bg,\n",
    "    )\n",
    "    # Get SHAP values\n",
    "    print('   Get SHAP values')\n",
    "    shap_values = explainer.shap_values(X_tf)\n",
    "    # Check if shap_values is array\n",
    "    if not isinstance(shap_values, np.ndarray):\n",
    "        shap_values = shap_values.values\n",
    "\n",
    "    # Aggregate all categories of onehot encoded features\n",
    "    if use_categorical_encoding:\n",
    "        # Get number of unique categories for each feature \n",
    "        n_categories = []\n",
    "        for feat in new_column_order[:-1]:\n",
    "            if feat in cat_feats:\n",
    "                n = X_smp[feat].nunique()\n",
    "                n_categories.append(n)\n",
    "            else:\n",
    "                n_categories.append(1)\n",
    "        # Sum SHAP values of all categories in each feature\n",
    "        new_shap_values = []\n",
    "        for values in shap_values:\n",
    "            # Split shap values into a list for each feature\n",
    "            values_split = np.split(values , np.cumsum(n_categories))\n",
    "            # Sum values within each list per feature\n",
    "            values_sum = [sum(l) for l in values_split]\n",
    "            new_shap_values.append(values_sum)\n",
    "        new_shap_values = np.array(new_shap_values)\n",
    "        # Replace SHAP values with new values\n",
    "        shap_values = new_shap_values\n",
    "\n",
    "    # Build results dictionary\n",
    "    shap_res = {}\n",
    "    shap_res['explainer'] = explainer\n",
    "    shap_res['shap_values'] = shap_values\n",
    "    shap_res['shap_feature_names'] = new_column_order\n",
    "    shap_res['X'] = X_smp\n",
    "    shap_res['y'] = y_smp\n",
    "    shap_res['X_transformed'] = X_tf\n",
    "    shap_res['y_pred'] = pd.Series(model.predict(X_smp), index=y_smp.index)\n",
    "    \n",
    "    return shap_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_explainer(model, X_trn, y_trn, X_tst, y_tst, sample_method='random', use_testdata=False, fit_preprocessor=True, sample_size=1000, background_size=20, feature_perturbation='interventional', ord_feats=[], cat_feats=[], drop_feats=[], random_seed=42):\n",
    "# https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/linear_models/Explaining%20a%20model%20that%20uses%20standardized%20features.html\n",
    "# Any univariate transformation applied to a model’s inputs does not effect the Shapley values for the model. Note that \n",
    "# multi-variate transformations like PCA decompositions do change the Shapley values so this trick does not apply there.\n",
    "    \n",
    "    # define 'column' order in np.array after preprocessing (1. numeric, 2. ordinal, 3. categorical)\n",
    "    ord_var_names = X_trn.columns[(X_trn.columns.isin(ord_feats)) & (~X_trn.columns.isin(drop_feats))].tolist()\n",
    "    cat_var_names = X_trn.columns[(X_trn.columns.isin(cat_feats)) & (~X_trn.columns.isin(drop_feats))].tolist()\n",
    "    num_var_names = X_trn.columns[(~X_trn.columns.isin(cat_var_names)) & (~X_trn.columns.isin(ord_var_names)) & (~X_trn.columns.isin(drop_feats))].tolist()\n",
    "    new_column_order = num_var_names\n",
    "    new_column_order.extend(ord_var_names)\n",
    "    new_column_order.extend(cat_var_names)\n",
    "    \n",
    "    # Use test or train data for sample dataset\n",
    "    if use_testdata:\n",
    "        X_smp = X_tst.copy()\n",
    "        y_smp = y_tst.copy()\n",
    "    else:\n",
    "        X_smp = X_trn.copy()\n",
    "        y_smp = y_trn.copy()\n",
    "    set_size = y_smp.shape[0]\n",
    "    # Use train data as background dataset\n",
    "    X_bg = X_trn.copy()\n",
    "    y_bg = y_trn.copy()\n",
    "\n",
    "    # Take sample from dataset\n",
    "    if (sample_size is not None):\n",
    "         if sample_size < X_smp.shape[0]:\n",
    "            if sample_method == 'random':\n",
    "                X_smp, _, y_smp, _ = train_test_split(X_smp, y_smp, train_size=min([sample_size, X_smp.shape[0]]), random_state=random_seed)\n",
    "            elif sample_method == 'stratified':\n",
    "                groups = create_bins(y_smp, num_bins=10, use_log=True)\n",
    "                X_smp, _, y_smp, _ = train_test_split(X_smp, y_smp, train_size=min([sample_size, X_smp.shape[0]]), stratify=groups, random_state=random_seed)\n",
    "\n",
    "    # Transform features\n",
    "    if 'preprocessor' in model.named_steps:\n",
    "        data_transformer = model.named_steps['preprocessor']\n",
    "        if fit_preprocessor:\n",
    "            data_transformer.fit(X_trn, y_trn)\n",
    "        transformer_uses_y = True\n",
    "        try:\n",
    "            data_transformer.transform(X_smp, y_smp)\n",
    "        except:\n",
    "            transformer_uses_y = False\n",
    "        if transformer_uses_y:\n",
    "            X_tf = data_transformer.transform(X_smp, y_smp)\n",
    "        else:\n",
    "            X_tf = data_transformer.transform(X_smp)\n",
    "    else:\n",
    "        X_tf = X_smp.values\n",
    "    # print(f\"   Column order of transformed data:\\n   {new_column_order}\")\n",
    "\n",
    "    # Make background dataset\n",
    "    if background_size is not None:\n",
    "        X_bg = X_bg.sample(n=min([background_size, X_bg.shape[0]]), random_state=random_seed)\n",
    "        y_bg = y_bg.sample(n=min([background_size, y_bg.shape[0]]), random_state=random_seed)\n",
    "    if 'preprocessor' in model.named_steps:\n",
    "        if transformer_uses_y:\n",
    "            X_bg = data_transformer.transform(X_bg, y_bg)\n",
    "        else:\n",
    "            X_bg = data_transformer.transform(X_bg)\n",
    "\n",
    "    # Check type of classifier\n",
    "    is_linear_classifier = True\n",
    "    try:\n",
    "        shap.LinearExplainer(model.named_steps['model'], X_bg)\n",
    "    except:\n",
    "        is_linear_classifier = False\n",
    "\n",
    "    # Explain predicted values\n",
    "    if not is_linear_classifier: \n",
    "        print('   Create TreeExplainer')\n",
    "        explainer = shap.TreeExplainer(\n",
    "            model.named_steps['model'], \n",
    "            X_bg if feature_perturbation=='interventional' else None,\n",
    "            feature_perturbation=feature_perturbation if feature_perturbation=='interventional' else 'correlation_dependent',\n",
    "        )\n",
    "    \n",
    "    elif is_linear_classifier:\n",
    "        print('   Create LinearExplainer')\n",
    "        explainer = shap.LinearExplainer(\n",
    "            model.named_steps['model'], \n",
    "            X_bg,\n",
    "            feature_perturbation=None if feature_perturbation=='interventional' else 'correlation_dependent',\n",
    "        )\n",
    "    # Get SHAP values\n",
    "    print('   Get SHAP values')\n",
    "    if 'assert_additivity' in dir(explainer):\n",
    "        shap_values = explainer(X_tf, check_additivity=False)\n",
    "    else:\n",
    "        shap_values = explainer(X_tf)\n",
    "    # Check if shap_values is array\n",
    "    if not isinstance(shap_values, np.ndarray):\n",
    "        shap_values = shap_values.values\n",
    "    # Get interaction values\n",
    "    if (feature_perturbation!='interventional') & (not is_linear_classifier):\n",
    "        interaction_values = explainer.shap_interaction_values(X_tf)\n",
    "    else:\n",
    "        interaction_values = None\n",
    "\n",
    "    # Aggregate all categories of onehot encoded features\n",
    "    if use_categorical_encoding:\n",
    "        # Get number of unique categories for each feature \n",
    "        n_categories = []\n",
    "        for feat in new_column_order[:-1]:\n",
    "            if feat in cat_feats:\n",
    "                n = X_smp[feat].nunique()\n",
    "                n_categories.append(n)\n",
    "            else:\n",
    "                n_categories.append(1)\n",
    "        \n",
    "        # Sum SHAP values of all categories in each feature\n",
    "        new_shap_values = []\n",
    "        for values in shap_values:\n",
    "            # Split shap values into list for each feature\n",
    "            values_split = np.split(values , np.cumsum(n_categories))\n",
    "            # Sum values within each list per feature\n",
    "            values_sum = [sum(l) for l in values_split]\n",
    "            new_shap_values.append(values_sum)\n",
    "        new_shap_values = np.array(new_shap_values)\n",
    "        \n",
    "        # Replace SHAP values with new values\n",
    "        shap_values = new_shap_values\n",
    "\n",
    "        # Apply vectorized operations to sum interaction values if they exist\n",
    "        if interaction_values is not None:\n",
    "            # Get number of unique categories for each feature\n",
    "            n_categories = []\n",
    "            for feat in new_column_order:\n",
    "                if feat in cat_feats:\n",
    "                    n = X_smp[feat].nunique()\n",
    "                    n_categories.append(n)\n",
    "                else:\n",
    "                    n_categories.append(1)\n",
    "\n",
    "            n_samples, _, _ = interaction_values.shape\n",
    "            n_agg_features = len(n_categories)\n",
    "            \n",
    "            # Create indexing arrays\n",
    "            cumsum_categories = np.cumsum([0] + n_categories)\n",
    "            \n",
    "            # Create new array for aggregated interaction values\n",
    "            new_interaction_values = np.zeros((n_samples, n_agg_features, n_agg_features))\n",
    "            \n",
    "            # Use np.add.at for efficient aggregation\n",
    "            for i in range(n_agg_features):\n",
    "                for j in range(n_agg_features):\n",
    "                    np.add.at(new_interaction_values[:, i, j],\n",
    "                              (),\n",
    "                              interaction_values[:, cumsum_categories[i]:cumsum_categories[i+1],\n",
    "                                                    cumsum_categories[j]:cumsum_categories[j+1]].sum(axis=(1,2)))\n",
    "            \n",
    "            # Replace SHAP interaction values with new values\n",
    "            interaction_values = new_interaction_values\n",
    "    \n",
    "    # Build results dictionary\n",
    "    shap_res = {}\n",
    "    shap_res['explainer'] = explainer\n",
    "    shap_res['shap_values'] = shap_values\n",
    "    if interaction_values is not None:\n",
    "        shap_res['interaction_values'] = interaction_values\n",
    "    shap_res['shap_feature_names'] = new_column_order\n",
    "    shap_res['X'] = X_smp\n",
    "    shap_res['y'] = y_smp\n",
    "    shap_res['X_transformed'] = X_tf\n",
    "    shap_res['y_pred'] = pd.Series(model.predict(X_smp), index=y_smp.index)\n",
    "\n",
    "    return shap_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters for SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_method = 'aci'\n",
    "\n",
    "shap_size = None\n",
    "shap_background_size = 100\n",
    "\n",
    "shap_sample_method = 'stratified'\n",
    "# shap_sample_method = 'random'\n",
    "\n",
    "# shap_feature_perturbation = 'interventional'\n",
    "shap_feature_perturbation = 'not interventional'\n",
    "\n",
    "method_for_invalid_classifiers = 'permutation'\n",
    "# method_for_invalid_classifiers = 'kernel'\n",
    "\n",
    "if pca_transform_numeric:\n",
    "    method_for_invalid_classifiers = 'kernel'\n",
    "\n",
    "cp_target_diff = False\n",
    "# cp_target_diff = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cp = X_new.copy().join(pred_int[cp_method]['interval'], how='left').rename(columns={'interval':'Conformal interval'})\n",
    "data_cp = data_cp.sort_values(['Location','Frequency','Time'])\n",
    "if cp_target_diff:\n",
    "    data_cp['Conformal interval'] = data_cp.groupby(['Location','Frequency'])['Conformal interval'].transform(lambda x: x.diff()).fillna(0)\n",
    "\n",
    "display(data_cp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate SHAP values per location-frequency combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_not_in_model_ = columns_not_in_model.copy()\n",
    "columns_not_in_model.extend(['Location','Frequency'])\n",
    "columns_not_in_model.extend(data_cp.filter(regex='lag=|MA=').columns)\n",
    "columns_not_in_model_cp = columns_not_in_model.copy()\n",
    "\n",
    "use_target_transform_ = use_target_transform\n",
    "use_target_transform = False\n",
    "\n",
    "rgr_mdl = XGBRegressor(objective='reg:squarederror', n_estimators=300, random_state=seed_value)\n",
    "# rgr_mdl = LGBMRegressor(n_estimators=50, force_col_wise=True, verbose=0, random_state=seed_value)\n",
    "# rgr_mdl = LinearRegression()\n",
    "\n",
    "# convert X_train columns to a list\n",
    "Xtrain_columns = data_cp.drop(columns=['Conformal interval']).columns.tolist()\n",
    "# set model pipeline\n",
    "pipe_cp = regression_model_pipeline()\n",
    "\n",
    "shap_values_cp = {}\n",
    "\n",
    "for loc_name in locations_for_conformal_prefit:\n",
    "    print(f\"Location: {loc_name}\")\n",
    "    shap_values_cp[loc_name] = {}\n",
    "    for i,freq in enumerate(X['Frequency'].sort_values().unique()):\n",
    "        print(f\" ({i+1}/{len(X['Frequency'].unique())}) Frequency: {freq}\")\n",
    "        X_cp = data_cp.copy().loc[(data_cp['Location'] == loc_name) & (data_cp['Frequency'] == freq)].drop(columns='Conformal interval')\n",
    "        y_cp = data_cp.copy().loc[(data_cp['Location'] == loc_name) & (data_cp['Frequency'] == freq),'Conformal interval']\n",
    "        \n",
    "        # split pipeline in preprocessor and model (works better for mapie 'aci' method)\n",
    "        preprocessor_cp = pipe_cp.named_steps['preprocessor']\n",
    "        model_cp = copy.deepcopy(pipe_cp.named_steps['model'])\n",
    "        # fit preprocessor before model training\n",
    "        preprocessor_cp.fit(X_train, y_train)\n",
    "        model_cp.fit(preprocessor_cp.transform(X_cp), y_cp)\n",
    "        pipe_cp_ = Pipeline(steps=[('preprocessor', preprocessor_cp), ('model', model_cp)])\n",
    "\n",
    "        # check if classifier works with the SHAP Explainer class\n",
    "        is_valid_shap_classifier = True\n",
    "        try:\n",
    "            shap.Explainer(\n",
    "                pipe_cp_.named_steps['model'], \n",
    "                pipe_cp_.named_steps['preprocessor'].fit_transform(X_cp, y_cp),\n",
    "            )\n",
    "        except:\n",
    "            is_valid_shap_classifier = False\n",
    "\n",
    "        if pca_transform_numeric:\n",
    "            is_valid_shap_classifier = False\n",
    "\n",
    "        # estimate SHAP values\n",
    "        if is_valid_shap_classifier:\n",
    "            print('   Uses SHAP Explainer')\n",
    "            shap_values_cp[loc_name][freq] = shap_explainer(\n",
    "                pipe_cp_, \n",
    "                X_trn=X_cp, \n",
    "                y_trn=y_cp,\n",
    "                X_tst=None,\n",
    "                y_tst=None,\n",
    "                use_testdata=False,\n",
    "                fit_preprocessor=False,\n",
    "                sample_method=shap_sample_method,\n",
    "                sample_size=shap_size,\n",
    "                background_size=shap_background_size,\n",
    "                feature_perturbation=shap_feature_perturbation,\n",
    "                cat_feats=categorical_columns if use_categorical_encoding else [None],\n",
    "                ord_feats=(discrete_to_ordinal + continuous_to_ordinal) if use_ordinal_encoding else [None],\n",
    "                drop_feats=columns_not_in_model_cp,\n",
    "                random_seed=seed_value,\n",
    "            )\n",
    "        else:\n",
    "            if method_for_invalid_classifiers == 'permutation':\n",
    "                print('   Uses SHAP Permutation Explainer')\n",
    "                shap_values_cp[loc_name][freq] = shap_permutation_explainer(\n",
    "                    pipe_cp_, \n",
    "                    X_trn=X_cp, \n",
    "                    y_trn=y_cp,\n",
    "                    X_tst=None,\n",
    "                    y_tst=None,\n",
    "                    use_testdata=False,\n",
    "                    sample_method=shap_sample_method,\n",
    "                    sample_size=shap_size,\n",
    "                    background_size=shap_background_size,\n",
    "                    cat_feats=categorical_columns if use_categorical_encoding else [None],\n",
    "                    ord_feats=(discrete_to_ordinal + continuous_to_ordinal) if use_ordinal_encoding else [None],\n",
    "                    drop_feats=columns_not_in_model_cp,\n",
    "                    random_seed=seed_value,\n",
    "                )\n",
    "            else:\n",
    "                print('   Uses SHAP Kernel Explainer')\n",
    "                shap_values_cp[loc_name][freq] = shap_kernel_explainer(\n",
    "                    pipe_cp_, \n",
    "                    X_trn=X_cp, \n",
    "                    y_trn=y_cp,\n",
    "                    X_tst=None,\n",
    "                    y_tst=None,\n",
    "                    use_testdata=False,\n",
    "                    sample_method=shap_sample_method,\n",
    "                    sample_size=shap_size,\n",
    "                    background_size=shap_background_size,\n",
    "                    k_kmeans=None,\n",
    "                    random_seed=seed_value,\n",
    "                )\n",
    "\n",
    "        shap_values_cp[loc_name][freq]['Time'] = X_cp.loc[shap_values_cp[loc_name][freq]['X'].index,'Time'].values\n",
    "        shap_values_cp[loc_name][freq]['Location'] = X_cp.loc[shap_values_cp[loc_name][freq]['X'].index,'Location'].values\n",
    "        shap_values_cp[loc_name][freq]['Frequency'] = X_cp.loc[shap_values_cp[loc_name][freq]['X'].index,'Frequency'].values\n",
    "\n",
    "use_target_transform = use_target_transform_\n",
    "columns_not_in_model = columns_not_in_model_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot SHAP of features over time for conformal intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_over_time(shap_data, freq, loc_name, win_size=6, max_features=10, cmap='tab10', skip_neg=False, color_other='lightgray', sort_statistic='max', show_features=True, cp_method='aci'):\n",
    "    \"\"\"\n",
    "    Function to plot SHAP values over time with the top N SHAP features stacked\n",
    "    and actual vs predicted values in a separate axis.\n",
    "    \n",
    "    Parameters:\n",
    "    - shap_values_cp: A dictionary containing SHAP values and other related data\n",
    "    - freq: The frequency to plot (index into the frequency dimension)\n",
    "    - loc_name: The location to plot (key to select the location from shap_values_cp)\n",
    "    - win_size: The size of the rolling window to smooth the SHAP values (default: 6)\n",
    "    - max_features: The maximum number of top SHAP features to display (default: 10)\n",
    "    - cmap: The colormap to use for feature plots (default: 'tab10')\n",
    "    - color_other: Color for the 'Other' category (default: light gray)\n",
    "    - sort_type: Type of sorting to use for the top features (default: 'max')\n",
    "    - show_features: Whether to show the features in a separate axis (default: True)\n",
    "    - cp_method: The method to used for calculating the prediction intervals (default: 'aci')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create DataFrame for SHAP values\n",
    "    shap_to_plot = pd.DataFrame(shap_data[loc_name][freq]['shap_values'], \n",
    "                                columns=shap_data[loc_name][freq]['shap_feature_names'])\n",
    "    shap_to_plot = shap_to_plot.assign(\n",
    "        **{\n",
    "            'Time': shap_data[loc_name][freq]['Time'],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Step 1: Calculate mean absolute SHAP values for each feature (sorted in descending order)\n",
    "    if sort_statistic == 'median':\n",
    "        sorted_shap_values = shap_to_plot.drop(columns=['Time']).abs().agg(np.median).sort_values(ascending=False)\n",
    "    else:\n",
    "        sorted_shap_values = shap_to_plot.drop(columns=['Time']).abs().agg(sort_statistic).sort_values(ascending=False)\n",
    "\n",
    "    # Step 2: Select top N features based on mean SHAP values\n",
    "    top_features = sorted_shap_values.index[:max_features].tolist()\n",
    "    other_features = sorted_shap_values.index[max_features:].tolist()\n",
    "    if len(other_features) == 1:\n",
    "        top_features.extend(other_features)\n",
    "        other_features = []\n",
    "\n",
    "    # Sort top features in ascending order by mean SHAP value\n",
    "    sorted_top_features = sorted_shap_values[top_features].sort_values().index\n",
    "\n",
    "    # Step 3: Aggregate other features into an 'Other' category\n",
    "    if len(other_features) > 0:\n",
    "        shap_to_plot['Other features'] = shap_to_plot[other_features].sum(axis='columns')\n",
    "\n",
    "    # Step 4: Create a new dataframe with only top N features and 'Other'\n",
    "    if len(other_features) > 0:\n",
    "        shap_top_N = shap_to_plot[['Time'] + ['Other features'] + list(sorted_top_features)]  # Sorted top features\n",
    "    else:\n",
    "        shap_top_N = shap_to_plot[['Time'] + list(sorted_top_features)]  # Sorted top features\n",
    "\n",
    "    # Step 5: Apply rolling window smoothing **before** splitting into positive and negative\n",
    "    for feature in shap_top_N.drop(columns='Time', errors='ignore').columns:  # Skip 'Time' column\n",
    "        shap_top_N.loc[:, feature] = shap_top_N[feature].rolling(window=win_size, center=True).mean()\n",
    "    \n",
    "    # Step 6: Split into positive and negative SHAP values **after rolling**\n",
    "    shap_pos = shap_top_N.copy()\n",
    "    shap_neg = shap_top_N.copy()\n",
    "\n",
    "    # Replace negative SHAP values with 0 in the positive dataframe, and vice versa\n",
    "    for feature in shap_top_N.drop(columns='Time', errors='ignore').columns:  # Skip 'Time' column\n",
    "        shap_pos[feature] = shap_top_N[feature].apply(lambda x: x if x > 0 else 0)\n",
    "        shap_neg[feature] = shap_top_N[feature].apply(lambda x: x if x < 0 else 0)\n",
    "\n",
    "    # Step 7: Define the colormap\n",
    "    color_map = plt.get_cmap(cmap)\n",
    "    num_features = len(shap_top_N.drop(columns='Time', errors='ignore').columns)  # Number of features to plot\n",
    "    if len(other_features) > 0:\n",
    "        colors = [color_other] + [color_map(i) for i in range(num_features - 1)][::-1]\n",
    "    else:\n",
    "        colors = [color_map(i) for i in range(num_features)][::-1]\n",
    "\n",
    "    # Step 8: Plot SHAP stacked area chart (Second Axis)\n",
    "    fig, ax = plt.subplots(nrows=3 if show_features else 2, ncols=1, sharex=True, figsize=(16,16) if show_features else (16, 12), dpi=150)\n",
    "    if show_features:\n",
    "        fig.subplots_adjust(hspace=0.4)\n",
    "    else:\n",
    "        fig.subplots_adjust(hspace=0.2)\n",
    "\n",
    "    # Initialize baselines for stacking (positive and negative)\n",
    "    pos_baseline = np.zeros(len(shap_pos['Time']))\n",
    "    neg_baseline = np.zeros(len(shap_neg['Time']))\n",
    "    # Add each feature to the plot\n",
    "    for i, feature in enumerate(shap_top_N.drop(columns='Time', errors='ignore').columns):  # Skip 'Time' column\n",
    "        # Stack positive SHAP values (upwards)\n",
    "        ax[0].fill_between(shap_pos['Time'], \n",
    "                           pos_baseline,  # Previous cumulative baseline\n",
    "                           pos_baseline + shap_pos[feature],  # New cumulative value\n",
    "                           label=feature, color=colors[i],\n",
    "                           linewidth=0.05)\n",
    "        # Update positive baseline\n",
    "        pos_baseline += shap_pos[feature]\n",
    "        \n",
    "        # Stack negative SHAP values (downwards)\n",
    "        if not skip_neg:\n",
    "            ax[0].fill_between(shap_neg['Time'], \n",
    "                               neg_baseline,  # Previous cumulative baseline\n",
    "                               neg_baseline + shap_neg[feature],  # New cumulative value\n",
    "                               label=None, color=colors[i],\n",
    "                               linewidth=0.05)\n",
    "            # Update negative baseline\n",
    "            neg_baseline += shap_neg[feature]\n",
    "\n",
    "    # Customize first axis\n",
    "    ax[0].grid(axis='both', color=[.7, .7, .7], linestyle='-', linewidth=.5)\n",
    "    ax[0].set_axisbelow(True)\n",
    "    ylabel = 'SHAP-based Feature Contributions\\nto Prediction Intervals'\n",
    "    if win_size > 1:\n",
    "        ylabel = ylabel + f\" (MA, window={win_size}hr)\"\n",
    "    ax[0].set_ylabel(ylabel, fontsize=12)\n",
    "\n",
    "    ax[0].set_ylabel(ylabel, fontsize=12)\n",
    "    ax[0].set_title(f\"Location: '{loc_name}', Frequency: {freq}\", fontsize=22)\n",
    "    ax[0].tick_params(axis='x', labeltop=False, labelbottom=True)\n",
    "\n",
    "    # Flip entire legend\n",
    "    handles, labels = ax[0].get_legend_handles_labels()\n",
    "    ax[0].legend(handles[::-1], labels[::-1], loc='upper left', bbox_to_anchor=(1., 1.02), ncol=1, fontsize=9)\n",
    "\n",
    "    # Step 9: Plot predicted vs actual values (First Axis)\n",
    "    ax[1].plot(shap_data[loc_name][freq]['Time'], \n",
    "               shap_data[loc_name][freq]['y'].rolling(window=win_size, center=True).mean(), \n",
    "               lw=2.5, label=f\"{cp_method.upper()} Interval Estimate\", zorder=0)\n",
    "    ax[1].plot(shap_data[loc_name][freq]['Time'], \n",
    "               shap_data[loc_name][freq]['y_pred'].rolling(window=win_size, center=True).mean(), \n",
    "               lw=1.5, label=\"Model-based Reconstruction\", zorder=10)\n",
    "    ax[1].grid(axis='both', color=[.7, .7, .7], linestyle='-', linewidth=.5)\n",
    "    ax[1].set_axisbelow(True)\n",
    "    ax[1].set_ylabel(f\"Estimated Prediction Intervals using {cp_method.upper()} Method\\nwith Step-wise Update of Residuals Distribution\", fontsize=12)\n",
    "    ax[1].legend(loc='upper left', bbox_to_anchor=(1., 1.02), ncol=1, fontsize=9)\n",
    "    ax[1].tick_params(axis='x', labeltop=False, labelbottom=True)\n",
    "\n",
    "    # Step 10: Show top N features\n",
    "    if show_features:\n",
    "        if len(other_features) > 0:\n",
    "            colors = colors[1:]\n",
    "        colors = colors[::-1]\n",
    "        for i,feature in enumerate(top_features):\n",
    "            ax[2].plot(shap_data[loc_name][freq]['X']['Time'], shap_data[loc_name][freq]['X'][feature] / shap_data[loc_name][freq]['X'][feature].abs().max(), lw=1.2, label=feature, color=colors[i])\n",
    "        ax[2].grid(axis='both', color=[.7, .7, .7], linestyle='-', linewidth=.5)\n",
    "        ax[2].set_axisbelow(True)\n",
    "        ax[2].set_ylabel('Normalized Feature Values', fontsize=12)\n",
    "        ax[2].legend(loc='upper left', bbox_to_anchor=(1., 1.02), ncol=1, fontsize=9)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_idx = 2\n",
    "freq_idx = 16\n",
    "win_size = 48\n",
    "max_features = 99\n",
    "\n",
    "freq = X['Frequency'].unique()[freq_idx]\n",
    "loc_name = locations_for_conformal_prefit[loc_idx]\n",
    "plot_shap_over_time(\n",
    "    shap_values_cp, \n",
    "    freq=freq, \n",
    "    loc_name=loc_name, \n",
    "    cmap='tab20', \n",
    "    win_size=win_size, \n",
    "    max_features=max_features, \n",
    "    color_other='lightgray',\n",
    "    sort_statistic='median',\n",
    "    show_features=False,\n",
    "    cp_method=cp_method,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'interaction_values' in shap_values_cp[loc_name][freq]:\n",
    "    shap.summary_plot(\n",
    "        shap_values_cp[loc_name][freq]['interaction_values'], \n",
    "        pd.DataFrame(shap_values_cp[loc_name][freq]['X'], columns=shap_values_cp[loc_name][freq]['shap_feature_names']), \n",
    "        max_display=len(shap_values_cp[loc_name][freq]['shap_feature_names'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get treatment effects of features for prediction intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_collinearity(dataf, feats_inmodel, X_coll=[], use_statsmodels=False):\n",
    "    X_vif = dataf.copy()[feats_inmodel]\n",
    "    X_vif = X_vif.drop(columns=X_coll, errors='ignore').dropna(axis='index')\n",
    "    cols_categorical = X_vif.columns[X_vif.dtypes=='category']\n",
    "    X_vif[cols_categorical] = X_vif[cols_categorical].apply(lambda x: x.cat.codes)\n",
    "    if use_statsmodels:\n",
    "        # VIF calculation with variance_inflation_factor function from statsmodels\n",
    "        # variance_inflation_factor expects the presence of a constant in the matrix of explanatory variables \n",
    "        # (https://stackoverflow.com/questions/42658379/variance-inflation-factor-in-python)\n",
    "        X_vif['Intercept'] = 1.\n",
    "        vif_info = pd.DataFrame()\n",
    "        vif_info = vif_info.assign(\n",
    "            **{\n",
    "                'Features':X_vif.columns,\n",
    "                'VIF':[variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])],\n",
    "            }\n",
    "        ).sort_values('VIF', ascending=False)\n",
    "        vif_info = vif_info[vif_info['Features']!='Intercept']\n",
    "    else:\n",
    "        # VIF calculation without variance_inflation_factor function from statsmodels\n",
    "        vif_info = np.diag(np.linalg.inv(np.corrcoef(X_vif.values, rowvar=False)))\n",
    "        vif_info = pd.DataFrame(vif_info, index=X_vif.columns).reset_index().rename(columns={'index':'Features',0:'VIF'})\n",
    "        vif_info = vif_info.sort_values('VIF', ascending=False)\n",
    "    return vif_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "def feature_variance_explained(X, y=None, cols_to_drop=[], add_target=False):\n",
    "    X_var = X.copy().drop(columns=cols_to_drop, errors='ignore')\n",
    "    if add_target & (y is not None):\n",
    "        X_var = X_var.assign(**{f\"{target} (=target)\":y})\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    for col in X_var.columns:\n",
    "        if X_var[col].apply(lambda x: isinstance(x, str)).all():\n",
    "            X_var[col] = label_encoder.fit_transform(X_var[col])\n",
    "    \n",
    "    var_explained = {}\n",
    "    for feat_name in X_var.columns:\n",
    "        var_explained[feat_name] = {}\n",
    "        for col in X_var.columns:\n",
    "            mdl = LinearRegression().fit(X_var[[col]].values, X_var[feat_name].values)\n",
    "            var_explained[feat_name][col] = np.sqrt(explained_variance_score(X_var[feat_name].values, mdl.predict(X_var[col].values.reshape(-1,1))))\n",
    "    \n",
    "    var_explained = pd.DataFrame(var_explained)\n",
    "    np.fill_diagonal(var_explained.values, np.nan)\n",
    "    \n",
    "    return var_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "columns_not_in_model_ = columns_not_in_model.copy()\n",
    "columns_not_in_model.extend(['Location','Frequency','Conformal interval'])\n",
    "columns_not_in_model.extend(data_cp.filter(regex='lag=|MA=').columns)\n",
    "columns_not_in_model_te = columns_not_in_model.copy()\n",
    "\n",
    "vif_info = check_collinearity(data_cp, data_cp.columns, X_coll=columns_not_in_model_te, use_statsmodels=False)\n",
    "display(vif_info)\n",
    "\n",
    "explained_var = feature_variance_explained(data_cp, y=data_cp['Conformal interval'], cols_to_drop=columns_not_in_model, add_target=False)\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "fig.subplots_adjust(bottom=0.05, top=0.92, left=0.1, right=0.9)\n",
    "fig.suptitle('Feature Variance Explained', fontsize=16)\n",
    "sns.heatmap(\n",
    "    explained_var, \n",
    "    ax=ax, \n",
    "    vmin=0, \n",
    "    vmax=1, \n",
    "    annot=True, \n",
    "    fmt='.3f', \n",
    "    cmap='Blues', \n",
    "    cbar=False, \n",
    "    linewidths=.5, \n",
    "    linecolor='gray', \n",
    "    annot_kws={'fontsize':8}\n",
    ")\n",
    "ax.set_xticks(ax.get_xticks())\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha='right')\n",
    "spine_color = 'gray'\n",
    "for spine in ['top', 'right', 'bottom', 'left']:\n",
    "    ax.spines[spine].set_visible(True)\n",
    "    ax.spines[spine].set_color(spine_color)\n",
    "    ax.spines[spine].set_linewidth(.7)\n",
    "plt.show()\n",
    "\n",
    "columns_not_in_model = columns_not_in_model_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters for DML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_kbins = False\n",
    "use_kbins = True\n",
    "\n",
    "nbins = 100\n",
    "\n",
    "if use_kbins:\n",
    "    cv_folds = 5\n",
    "else:\n",
    "    cv_folds = 1\n",
    "\n",
    "dml_type = 'linear'\n",
    "# dml_type = 'forest'\n",
    "# dml_type = 'orthogonal'\n",
    "\n",
    "# cv_sampling = 'random'\n",
    "cv_sampling = 'stratified'\n",
    "\n",
    "cv_nbins = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate individual treatment effects per location-frequency combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_not_in_model_ = columns_not_in_model.copy()\n",
    "columns_not_in_model.extend(['Location','Frequency'])\n",
    "columns_not_in_model.extend(data_cp.filter(regex='lag=|MA=').columns)\n",
    "columns_not_in_model_te = columns_not_in_model.copy()\n",
    "\n",
    "use_target_transform_ = use_target_transform\n",
    "use_target_transform = False\n",
    "\n",
    "rgr_mdl = XGBRegressor(objective='reg:squarederror', n_estimators=300, random_state=seed_value)\n",
    "# rgr_mdl = LGBMRegressor(n_estimators=50, force_col_wise=True, verbose=0, random_state=seed_value)\n",
    "# rgr_mdl = LinearRegression()\n",
    "\n",
    "# convert X_train columns to a list\n",
    "Xtrain_columns = data_cp.drop(columns=['Conformal interval']).columns.tolist()\n",
    "# set model pipeline\n",
    "pipe_te = regression_model_pipeline()\n",
    "\n",
    "ite_values_cp = {}\n",
    "\n",
    "for loc_name in locations_for_conformal_prefit:\n",
    "    print(f\"Location: {loc_name}\")\n",
    "    ite_values_cp[loc_name] = {}\n",
    "    for i,freq in enumerate(X['Frequency'].sort_values().unique()):\n",
    "        ite_values_cp[loc_name][freq] = {}\n",
    "        print(f\"  ({i+1}/{len(X['Frequency'].unique())}) Frequency: {freq}\")\n",
    "        X_te = data_cp.copy().loc[(data_cp['Location'] == loc_name) & (data_cp['Frequency'] == freq)].drop(columns='Conformal interval')\n",
    "        y_te = data_cp.copy().loc[(data_cp['Location'] == loc_name) & (data_cp['Frequency'] == freq),'Conformal interval']\n",
    "        \n",
    "        # split pipeline in preprocessor and model\n",
    "        preprocessor_te = pipe_te.named_steps['preprocessor']\n",
    "        outcome_model = pipe_te.named_steps['model']\n",
    "        # fit preprocessor before model training\n",
    "        preprocessor_te.fit(X_train, y_train);\n",
    "        X_te_tf = pd.DataFrame(preprocessor_te.transform(X_te), columns=X_te.drop(columns=columns_not_in_model_te).columns)\n",
    "        # fit outcome model\n",
    "        outcome_model.fit(X_te_tf, y_te)\n",
    "        # build treatment model\n",
    "        treatment_model = XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=seed_value)\n",
    "        if dml_type == 'orthogonal':\n",
    "            treatment_model = Ridge()\n",
    "\n",
    "        if (cv_sampling == 'stratified') & (use_kbins):\n",
    "            y_binned = pd.qcut(y_te, q=cv_nbins, labels=False)\n",
    "            skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=seed_value)\n",
    "            folds = [(train_idx, test_idx) for train_idx, test_idx in skf.split(X_te, y_binned)]\n",
    "        else:\n",
    "            folds = cv_folds\n",
    "\n",
    "        # initialize DML for continuous treatment and outcome\n",
    "        if dml_type == 'linear':\n",
    "            dml = LinearDML(model_y=outcome_model, model_t=treatment_model, cv=folds)\n",
    "        elif dml_type == 'forest':\n",
    "            dml = CausalForestDML(model_y=outcome_model, model_t=treatment_model, cv=folds)\n",
    "        elif dml_type == 'orthogonal':\n",
    "            dml = DMLOrthoForest(model_Y=outcome_model, model_T=treatment_model, global_res_cv=folds)\n",
    "        \n",
    "        treatment_effect, lower_bound, upper_bound = {}, {}, {}\n",
    "        for feature in tqdm(X_te_tf.columns):\n",
    "            T = X_te_tf.copy()[feature]\n",
    "            # discretize treatment in N bins\n",
    "            if use_kbins:\n",
    "                kbins = KBinsDiscretizer(n_bins=nbins, encode='ordinal', strategy='uniform')\n",
    "                T = kbins.fit_transform(T.values.reshape(-1, 1)).flatten()\n",
    "            X_covariates = X_te_tf.drop(columns=[feature])\n",
    "            # fit DML model with the outcome (Y), treatment (T), and covariates (X)\n",
    "            dml.fit(Y=y_te, T=T, X=X_covariates)\n",
    "            treatment_effect[feature] = dml.effect(X_covariates)\n",
    "            lower_bound[feature] = dml.effect_interval(X_covariates, alpha=0.05)[0]\n",
    "            upper_bound[feature] = dml.effect_interval(X_covariates, alpha=0.05)[1]\n",
    "        treatment_effect = pd.DataFrame(treatment_effect, index=X_te.index)\n",
    "        upper_bound = pd.DataFrame(upper_bound, index=X_te.index)\n",
    "        lower_bound = pd.DataFrame(lower_bound, index=X_te.index)\n",
    "\n",
    "        ite_values_cp[loc_name][freq]['treatment_effect'] = treatment_effect\n",
    "        ite_values_cp[loc_name][freq]['upper_bound'] = upper_bound\n",
    "        ite_values_cp[loc_name][freq]['lower_bound'] = lower_bound\n",
    "        ite_values_cp[loc_name][freq]['X'] = X_te\n",
    "        ite_values_cp[loc_name][freq]['y'] = y_te\n",
    "        ite_values_cp[loc_name][freq]['y_pred'] = pd.Series(outcome_model.predict(X_te_tf), index=X_te.index)\n",
    "\n",
    "        ite_values_cp[loc_name][freq]['Time'] = X_te['Time'].values\n",
    "        ite_values_cp[loc_name][freq]['Location'] = X_te['Location'].values\n",
    "        ite_values_cp[loc_name][freq]['Frequency'] = X_te['Frequency'].values\n",
    "\n",
    "use_target_transform = use_target_transform_\n",
    "columns_not_in_model = columns_not_in_model_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot treatment effects over time for conformal intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ite_over_time(ite_data, freq, loc_name, win_size=6, max_features=10, cmap='tab10', skip_neg=False, color_other='lightgray', sort_statistic='max', show_features=True, effect_type='raw'):\n",
    "    \"\"\"\n",
    "    Function to plot SHAP values over time with the top N SHAP features stacked\n",
    "    and actual vs predicted values in a separate axis.\n",
    "    \n",
    "    Parameters:\n",
    "    - ite_values_cp: A dictionary containing SHAP values and other related data\n",
    "    - freq: The frequency to plot (index into the frequency dimension)\n",
    "    - loc_name: The location to plot (key to select the location from ite_values_cp)\n",
    "    - win_size: The size of the rolling window to smooth the SHAP values (default: 6)\n",
    "    - max_features: The maximum number of top SHAP features to display (default: 10)\n",
    "    - cmap: The colormap to use for feature plots (default: 'tab10')\n",
    "    - color_other: Color for the 'Other' category (default: light gray)\n",
    "    - sort_type: Type of sorting to use for the top features (default: 'max')\n",
    "    - show_features: Whether to show the features in a separate axis (default: True)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get DataFrame for treatment effects\n",
    "    ite_to_plot = ite_data[loc_name][freq].copy()['treatment_effect']\n",
    "    ite_to_plot = ite_to_plot.assign(\n",
    "        **{\n",
    "            'Time': ite_data[loc_name][freq]['Time'],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if effect_type == 'proportional':\n",
    "        threshold = 1e-3\n",
    "        for feature in ite_to_plot.drop(columns='Time', errors='ignore').columns:\n",
    "            ite_to_plot[feature] = np.where(ite_data[loc_name][freq]['X'][feature] > threshold, ite_to_plot[feature] / ite_data[loc_name][freq]['X'][feature], 0)\n",
    "    elif effect_type == 'signed':\n",
    "        for feature in ite_to_plot.drop(columns='Time', errors='ignore').columns:\n",
    "            ite_to_plot[feature] = ite_to_plot[feature] * np.sign(ite_data[loc_name][freq]['X'][feature])\n",
    "\n",
    "    # Step 1: Calculate mean absolute treatment effect for each feature (sorted in descending order)\n",
    "    if sort_statistic == 'median':\n",
    "        sorted_ite_values = ite_to_plot.drop(columns=['Time']).abs().agg(np.median).sort_values(ascending=False)\n",
    "    else:\n",
    "        sorted_ite_values = ite_to_plot.drop(columns=['Time']).abs().agg(sort_statistic).sort_values(ascending=False)\n",
    "\n",
    "    # Step 2: Select top N features based on mean SHAP values\n",
    "    top_features = sorted_ite_values.index[:max_features].tolist()\n",
    "    other_features = sorted_ite_values.index[max_features:].tolist()\n",
    "    if len(other_features) == 1:\n",
    "        top_features.extend(other_features)\n",
    "        other_features = []\n",
    "\n",
    "    # Sort top features in ascending order by mean treatment effect\n",
    "    sorted_top_features = sorted_ite_values[top_features].sort_values().index\n",
    "\n",
    "    # Step 4: Create a new dataframe with only top N features and 'Other'\n",
    "    ite_top_N = ite_to_plot[['Time'] + list(sorted_top_features)]  # Sorted top features\n",
    "\n",
    "    # Step 5: Apply rolling window smoothing **before** splitting into positive and negative\n",
    "    for feature in ite_top_N.drop(columns='Time', errors='ignore').columns:  # Skip 'Time' column\n",
    "        ite_top_N.loc[:, feature] = ite_top_N[feature].rolling(window=win_size, center=True).mean()\n",
    "    \n",
    "    # Step 6: Split into positive and negative SHAP values **after rolling**\n",
    "    ite_pos = ite_top_N.copy()\n",
    "    ite_neg = ite_top_N.copy()\n",
    "\n",
    "    # Replace negative SHAP values with 0 in the positive dataframe, and vice versa\n",
    "    for feature in ite_top_N.drop(columns='Time', errors='ignore').columns:  # Skip 'Time' column\n",
    "        ite_pos[feature] = ite_top_N[feature].apply(lambda x: x if x > 0 else 0)\n",
    "        ite_neg[feature] = ite_top_N[feature].apply(lambda x: x if x < 0 else 0)\n",
    "\n",
    "    # Step 7: Define the colormap\n",
    "    color_map = plt.get_cmap(cmap)\n",
    "    num_features = len(ite_top_N.drop(columns='Time', errors='ignore').columns)  # Number of features to plot\n",
    "    colors = [color_map(i) for i in range(num_features)][::-1]\n",
    "\n",
    "    # Step 8: Plot SHAP stacked area chart (Second Axis)\n",
    "    fig, ax = plt.subplots(nrows=3 if show_features else 2, ncols=1, sharex=True, figsize=(16,16) if show_features else (16, 12), dpi=150)\n",
    "    if show_features:\n",
    "        fig.subplots_adjust(hspace=0.4)\n",
    "    else:\n",
    "        fig.subplots_adjust(hspace=0.2)\n",
    "\n",
    "    # Initialize baselines for stacking (positive and negative)\n",
    "    pos_baseline = np.zeros(len(ite_pos['Time']))\n",
    "    neg_baseline = np.zeros(len(ite_neg['Time']))\n",
    "    # Add each feature to the plot\n",
    "    for i, feature in enumerate(ite_top_N.drop(columns='Time', errors='ignore').columns):  # Skip 'Time' column\n",
    "        # Stack positive SHAP values (upwards)\n",
    "        ax[0].fill_between(ite_pos['Time'], \n",
    "                           pos_baseline,  # Previous cumulative baseline\n",
    "                           pos_baseline + ite_pos[feature],  # New cumulative value\n",
    "                           label=feature, color=colors[i],\n",
    "                           linewidth=0.05)\n",
    "        # Update positive baseline\n",
    "        pos_baseline += ite_pos[feature]\n",
    "        \n",
    "        # Stack negative SHAP values (downwards)\n",
    "        if not skip_neg:\n",
    "            ax[0].fill_between(ite_neg['Time'], \n",
    "                               neg_baseline,  # Previous cumulative baseline\n",
    "                               neg_baseline + ite_neg[feature],  # New cumulative value\n",
    "                               label=None, color=colors[i],\n",
    "                               linewidth=0.05)\n",
    "            # Update negative baseline\n",
    "            neg_baseline += ite_neg[feature]\n",
    "\n",
    "    # Customize first axis\n",
    "    ax[0].grid(axis='both', color=[.7, .7, .7], linestyle='-', linewidth=.5)\n",
    "    ax[0].set_axisbelow(True)\n",
    "    if effect_type == 'proportional':\n",
    "        ylabel = 'Proportional Treatment Effects of Features\\non Prediction Intervals'\n",
    "    elif effect_type == 'signed':\n",
    "        ylabel = 'Signed Treatment Effects of Features\\non Prediction Intervals'\n",
    "    else:\n",
    "        ylabel = 'Raw Treatment Effects of Features \\non Prediction Intervals'\n",
    "    if win_size > 1:\n",
    "        ylabel = ylabel + f\" (MA, window={win_size}hr)\"\n",
    "    ax[0].set_ylabel(ylabel, fontsize=12)\n",
    "\n",
    "    ax[0].set_title(f\"Location: '{loc_name}', Frequency: {freq}\", fontsize=22)\n",
    "    ax[0].tick_params(axis='x', labeltop=False, labelbottom=True)\n",
    "    ax[0].annotate(\n",
    "        f\"Note: Treatment effects aren't necessarily additive. The effect of changing multiple features\\nsimultaneously might not be the sum of their individual effects, especially if features interact\", \n",
    "        xy=(0.01, 0.02), xycoords='axes fraction', fontsize=10, fontstyle='oblique'\n",
    "    )\n",
    "\n",
    "    # Flip entire legend\n",
    "    handles, labels = ax[0].get_legend_handles_labels()\n",
    "    ax[0].legend(handles[::-1], labels[::-1], loc='upper left', bbox_to_anchor=(1., 1.02), ncol=1, fontsize=9)\n",
    "\n",
    "    # Step 9: Plot predicted vs actual values (First Axis)\n",
    "    ax[1].plot(ite_data[loc_name][freq]['Time'], \n",
    "               ite_data[loc_name][freq]['y'].rolling(window=win_size, center=True).mean(), \n",
    "               lw=2.5, label=f\"{cp_method.upper()} Interval Estimate\", zorder=0)\n",
    "    ax[1].plot(ite_data[loc_name][freq]['Time'], \n",
    "               ite_data[loc_name][freq]['y_pred'].rolling(window=win_size, center=True).mean(), \n",
    "               lw=1.5, label=\"Model-based Reconstruction\", zorder=10)\n",
    "    ax[1].grid(axis='both', color=[.7, .7, .7], linestyle='-', linewidth=.5)\n",
    "    ax[1].set_axisbelow(True)\n",
    "    ax[1].set_ylabel(f\"Estimated Prediction Intervals using {cp_method.upper()} Method\\nwith Step-wise Update of Residuals Distribution\", fontsize=12)\n",
    "    ax[1].legend(loc='upper left', bbox_to_anchor=(1., 1.02), ncol=1, fontsize=9)\n",
    "    ax[1].tick_params(axis='x', labeltop=False, labelbottom=True)\n",
    "\n",
    "    # Step 10: Show top N features\n",
    "    if show_features:\n",
    "        if len(other_features) > 0:\n",
    "            colors = colors[1:]\n",
    "        colors = colors[::-1]\n",
    "        for i,feature in enumerate(top_features):\n",
    "            ax[2].plot(ite_data[loc_name][freq]['X']['Time'], ite_data[loc_name][freq]['X'][feature] / ite_data[loc_name][freq]['X'][feature].abs().max(), lw=1.2, label=feature, color=colors[i])\n",
    "        ax[2].grid(axis='both', color=[.7, .7, .7], linestyle='-', linewidth=.5)\n",
    "        ax[2].set_axisbelow(True)\n",
    "        ax[2].set_ylabel('Normalized Feature Values', fontsize=12)\n",
    "        ax[2].legend(loc='upper left', bbox_to_anchor=(1., 1.02), ncol=1, fontsize=9)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_idx = 2\n",
    "freq_idx = 16\n",
    "win_size = 48\n",
    "max_features = 99\n",
    "\n",
    "freq = X['Frequency'].unique()[freq_idx]\n",
    "loc_name = locations_for_conformal_prefit[loc_idx]\n",
    "plot_ite_over_time(\n",
    "    ite_values_cp, \n",
    "    freq=freq, \n",
    "    loc_name=loc_name, \n",
    "    cmap='tab20', \n",
    "    win_size=win_size, \n",
    "    max_features=max_features, \n",
    "    sort_statistic='median',\n",
    "    show_features=False,\n",
    "    effect_type='raw',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swan-xai (py3.9)",
   "language": "python",
   "name": "swan-xai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
